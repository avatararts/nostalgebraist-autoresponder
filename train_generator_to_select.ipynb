{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oksd9Gl8pbXY"
   },
   "source": [
    "Train the selector model on Google Colab (more recent approach)\n",
    "\n",
    "Unlike `train_selector.ipynb`, this trains a model whose inputs are layer activations from the generator model, rather than a model whose inputs are the text which the generator has created.  Thus it needs access to the generator model, and is coupled to it.  (If you re-train the generator, you must re-train this model on the new one.)\n",
    "\n",
    "Similar assumptions about your environment to those in `generator.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "8QywdYuw7LC4",
    "outputId": "916f4ae4-ce60-4d88-9b41-14590f567ce1"
   },
   "outputs": [],
   "source": [
    "# make sure you have a GPU with ~16GB memory, for 1558M\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a8jgEx0wvX8I",
    "outputId": "65db585a-4032-4ad6-eeae-13b768001f2e"
   },
   "outputs": [],
   "source": [
    "# cell for Colab-specific stuff, if you're on Colab\n",
    "\n",
    "# if you're using Google Colab, need to tell it not to use tf 2.x\n",
    "%tensorflow_version 1.x\n",
    "\n",
    "# if you're using Google Colab + Google Drive, mount and change dir\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd \"/content/drive/My Drive/\"\n",
    "\n",
    "# i assume you're now somewhere with a directory called \"gpt-2\" holding my gpt-2 fork\n",
    "# and your fine-tuned model\n",
    "%cd \"gpt-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lWleFcgvkJ5"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load/prep training corpus -- same as in `train_selector.ipynb` (TODO: DRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6UDjBhQvnRd"
   },
   "outputs": [],
   "source": [
    "# you should have created this file by scraping posts/note counts, and put it on google drive\n",
    "data_path = \"reward/reward.pkl.gz\"\n",
    "with open(data_path, \"rb\") as f:\n",
    "    ids_to_reward_data = pickle.load(f)[\"ids_to_reward_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfIQ5BLUvuNH"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def inverse_format_post_for_api(post):\n",
    "    if post.startswith(\"<p>\"):\n",
    "        post = post[len(\"<p>\"):]\n",
    "    if post.endswith(\"</p>\"):\n",
    "        post = post[:-len(\"</p>\")]\n",
    "    # post = post.lstrip(\"<p>\").rstrip(\"</p>\")\n",
    "    post = re.sub(r\"</p><p>\", \"\\n\", post)\n",
    "    post = re.sub(r\"<br>\", \"\\n\", post)\n",
    "    return post\n",
    "\n",
    "def make_train_data(ids_to_reward_data, continuation_only=True, prompt_as_col=True):\n",
    "    train_data = []\n",
    "    for k, v in ids_to_reward_data.items():\n",
    "      if v.get(\"note_count\") is None or v.get('continuation') is None:\n",
    "        continue\n",
    "      if continuation_only:\n",
    "        train_data.append([k, v[\"continuation\"], v[\"note_count\"]])\n",
    "      else:\n",
    "        train_data.append([k, \" \".join(v[\"prompt\"].split(\" \")[-64:]) + v[\"continuation\"], v[\"note_count\"]])\n",
    "      if prompt_as_col:\n",
    "        train_data[-1].append(v[\"prompt\"])\n",
    "        \n",
    "    if prompt_as_col:\n",
    "      train_data = pd.DataFrame(train_data, columns=[\"id\", \"text\", \"note_count\", \"prompt\"])\n",
    "    else:\n",
    "      train_data = pd.DataFrame(train_data, columns=[\"id\", \"text\", \"note_count\"])\n",
    "\n",
    "    train_data.text = train_data.text.apply(inverse_format_post_for_api)\n",
    "    train_data.text = train_data.text.apply(lambda s: s.lstrip(\"\\n\"))\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "rY2PRQyNvunB",
    "outputId": "bf752dfe-0304-4ddf-83bc-2fc06b362820"
   },
   "outputs": [],
   "source": [
    "train_data = make_train_data(ids_to_reward_data, continuation_only=True)\n",
    "train_data.note_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5MitKP0vvxL"
   },
   "outputs": [],
   "source": [
    "temporally_ordered_train_data = train_data.sort_values(by=\"id\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "id": "s956ymS9vwps",
    "outputId": "02c24f44-5a47-411f-c791-992b453230fc"
   },
   "outputs": [],
   "source": [
    "def non_overlapping_ma(array, width=31):\n",
    "  return pd.Series([np.average(array[ix:ix+width], )\n",
    "   for ix in range(0, len(array), width)])\n",
    "\n",
    "window_width = 140\n",
    "window_halfw = window_width//2\n",
    "\n",
    "skip_n_most_recent = 40\n",
    "\n",
    "allow_partial_windows = False\n",
    "window_frac_left = 0.8 # None\n",
    "\n",
    "rolling_quantiles = {}\n",
    "rolling_advantages = {}\n",
    "\n",
    "if window_frac_left is not None:\n",
    "  window_shift_left = -1*int(window_frac_left*window_width)\n",
    "  window_shift_right = window_width + window_shift_left\n",
    "else:\n",
    "  window_shift_left = -window_halfw\n",
    "  window_shift_right = window_halfw\n",
    "\n",
    "last_ix_allowed = len(temporally_ordered_train_data) - skip_n_most_recent\n",
    "\n",
    "if allow_partial_windows:\n",
    "  ixs = temporally_ordered_train_data.index[:last_ix_allowed]\n",
    "else:\n",
    "  ixs = temporally_ordered_train_data.index[(0-window_shift_left):(last_ix_allowed-window_shift_right)]\n",
    "\n",
    "print(f\"using ({ixs.min()} to {ixs.max()}) of (0 to {len(temporally_ordered_train_data)-1})\")\n",
    "\n",
    "for ix in ixs:\n",
    "  point = temporally_ordered_train_data.loc[ix, 'note_count']\n",
    "  window = temporally_ordered_train_data.loc[ix+window_shift_left:ix+window_shift_right, 'note_count']\n",
    "  rolling_quantiles[ix] = (point>=window).mean()\n",
    "  rolling_advantages[ix] = (point-window).mean()\n",
    "\n",
    "rolling_quantiles = pd.Series(rolling_quantiles)\n",
    "rolling_advantages = pd.Series(rolling_advantages)\n",
    "\n",
    "non_overlapping_ma(rolling_quantiles, width=21).plot(lw=1, ls='--', marker='.', markersize=5, figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hkPhy1IyvxVE"
   },
   "outputs": [],
   "source": [
    "use_mov_avg = True\n",
    "notes_key = \"rolling_quantile\" if use_mov_avg else \"note_count\"\n",
    "\n",
    "if use_mov_avg:\n",
    "  train_data_ = temporally_ordered_train_data.loc[rolling_quantiles.index]\n",
    "  train_data_[\"rolling_quantile\"] = rolling_quantiles\n",
    "  train_data_[\"rolling_advantage\"] = rolling_advantages\n",
    "else:\n",
    "  train_data_ = temporally_ordered_train_data\n",
    "\n",
    "regression = False\n",
    "drop_midrange = True\n",
    "smaller_midrange_dropped = False\n",
    "\n",
    "reg_log = False\n",
    "reg_cutoff = 30\n",
    "\n",
    "continuation_only = True\n",
    "\n",
    "#train_data = make_train_data(ids_to_reward_data, continuation_only=continuation_only)\n",
    "\n",
    "if regression:\n",
    "  notes_key = \"rolling_advantage\" if use_mov_avg else \"note_count\"\n",
    "  if reg_log:\n",
    "    logstart = 1-train_data_[notes_key].min()\n",
    "    train_data_[\"target\"] = train_data_[notes_key].apply(lambda x: np.log(x+logstart))\n",
    "  elif reg_cutoff:\n",
    "    train_data_[\"target\"] = train_data_[notes_key].apply(lambda x: min(x, reg_cutoff))\n",
    "  else:\n",
    "    train_data_[\"target\"] = train_data_[notes_key]\n",
    "\n",
    "  stratify = None\n",
    "elif drop_midrange and not use_mov_avg:\n",
    "  train_data_[\"target\"] = (train_data_[notes_key]>=4).astype(int)\n",
    "  train_data_ = train_data_[(train_data_[notes_key] <= 1) | (train_data_[notes_key] >=4)]\n",
    "  stratify = train_data_[\"target\"]\n",
    "elif drop_midrange and use_mov_avg:\n",
    "  if smaller_midrange_dropped:\n",
    "    MIDRANGE_BOTTOM = np.percentile(train_data_[notes_key], 30)\n",
    "    MIDRANGE_TOP = np.percentile(train_data_[notes_key], 70)\n",
    "  else:\n",
    "    MIDRANGE_BOTTOM = np.percentile(train_data_[notes_key], 24)\n",
    "    MIDRANGE_TOP = np.percentile(train_data_[notes_key], 76)\n",
    "\n",
    "  train_data_[\"target\"] = (train_data_[notes_key] >= MIDRANGE_TOP).astype(int)\n",
    "  train_data_ = train_data_[(train_data_[notes_key] <= MIDRANGE_BOTTOM) | (train_data_[notes_key] >= MIDRANGE_TOP)]\n",
    "  stratify = train_data_[\"target\"]\n",
    "else:\n",
    "# split at middle\n",
    "  train_data_[\"target\"] = (train_data_[notes_key] > 2).astype(int)\n",
    "  train_data_ = train_data_\n",
    "  stratify = train_data_[\"target\"]\n",
    "\n",
    "\n",
    "model_inputs = train_data_[[\"text\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "OXFB0C0Wvy7u",
    "outputId": "36ebaab7-100d-4086-ce59-53909e006c58"
   },
   "outputs": [],
   "source": [
    "model_inputs.target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S-8eTBKCvz5P",
    "outputId": "395da4f9-f6ec-43ca-9e75-d2470b1521d8"
   },
   "outputs": [],
   "source": [
    "def baserate_loss(target):\n",
    "  baserate = np.mean(target)\n",
    "\n",
    "  return -1 * (baserate*np.log(baserate) + (1-baserate)*np.log(1-baserate))\n",
    "\n",
    "print(f\"baserate_loss (all): {baserate_loss(model_inputs.target):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generic tensorflow utility code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkzg-s6DwLpw"
   },
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess, print_names=True):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "\n",
    "    if print_names:\n",
    "      for i in not_initialized_vars:\n",
    "        print(i.name)\n",
    "\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "\n",
    "def re_initialize(sess, var_names):\n",
    "  sess.run(tf.variables_initializer(var_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eusbjBiwwAYM"
   },
   "source": [
    "selector/generator model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5WVHXU9Rv2Wk",
    "outputId": "c66b7bc0-3f64-4e37-9bda-9de44c11ed65"
   },
   "outputs": [],
   "source": [
    "%pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMF3foFswB7Z"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TOarva91wDPD",
    "outputId": "65ad5321-14bb-4ba0-dcdb-3ae642bf7121"
   },
   "outputs": [],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import model, sample, encoder\n",
    "from load_dataset import load_dataset, Sampler\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hqkv8ClAwD68"
   },
   "outputs": [],
   "source": [
    "model_name = \"\"  # fill in -- should be a directory under /models\n",
    "\n",
    "enc = encoder.get_encoder(model_name, eot_workaround=True)\n",
    "hparams = model.default_hparams()\n",
    "with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
    "    hparams.override_from_dict(json.load(f))\n",
    "\n",
    "hparams.set_hparam(\"attn_dropout\", 0)\n",
    "hparams.set_hparam(\"res_dropout\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1xamAIk0G6K"
   },
   "outputs": [],
   "source": [
    "length=825\n",
    "required_continuation_room = 100\n",
    "max_context_size = length - required_continuation_room\n",
    "\n",
    "# \"for_h\" doesn't mean anything here (\"it's historical\"), this is just the batch size\n",
    "batch_size_for_h = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4u6Hf8O8wIzJ",
    "outputId": "da199eed-0aa1-469a-e349-ab97aad2e5ea"
   },
   "outputs": [],
   "source": [
    "SELECTION_CHAR = \"<|endoftext|>\"\n",
    "print(enc.encode(SELECTION_CHAR))\n",
    "SELECTION_TOK = enc.encode(SELECTION_CHAR)[-1]\n",
    "print(SELECTION_TOK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JbHQlID_wJxh"
   },
   "source": [
    "define model architecture\n",
    "\n",
    "much of this either reuses gpt2 code (imported in `from model import *`), or defines slightly modified equivalents of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1x6oY3k1Oi0v"
   },
   "outputs": [],
   "source": [
    "from model import *\n",
    "\n",
    "def extract_selection_ix(tokens, extract_from):\n",
    "  mask = tf.equal(tf.dtypes.cast(tokens, tf.int32), SELECTION_TOK)\n",
    "  extracted_ragged = tf.ragged.boolean_mask(extract_from, mask)\n",
    "  \n",
    "  row_lengths = extracted_ragged.row_lengths()\n",
    "  row_ixs = row_lengths-1\n",
    "  selection_ix = tf.stack([tf.range(0, batch_size_for_h, dtype=tf.int64), row_ixs], axis=1,)\n",
    "\n",
    "  extracted = tf.gather_nd(extracted_ragged.to_tensor(), selection_ix,  )\n",
    "\n",
    "  return {\"extracted\": extracted, \"selection_ix\": selection_ix}\n",
    "\n",
    "def model_activations(hparams, X, hparams_select, \n",
    "                      layer_nums: list, \n",
    "                      norm_layers_after: bool=False,\n",
    "                      past=None, past_select=None,\n",
    "                      scope='model', reuse=tf.AUTO_REUSE):\n",
    "    activations = []\n",
    "    h_names = []\n",
    "    \n",
    "    dtype = hparams.dtype if hparams else tf.float32\n",
    "    with tf.variable_scope(scope, reuse=reuse, dtype=dtype):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wpe = get_variable('wpe') or tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.01, dtype=dtype))\n",
    "        wte = get_variable('wte') or tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02, dtype=dtype))\n",
    "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "            if layer in layer_nums:\n",
    "              h_name = f'h{layer}'\n",
    "              print(f'{h_name} found')\n",
    "              h_names.append(h_name)\n",
    "              activations.append(h)\n",
    "\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f', hparams=hparams)\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "\n",
    "        # activations\n",
    "        if norm_layers_after:\n",
    "          activations = [norm(act, f'ln_after_{act_name}', hparams=hparams_select)\n",
    "                         for act_name, act in zip(h_names, activations)]\n",
    "\n",
    "        results['activations'] = list(zip(h_names, activations))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OhmC45uhu4W"
   },
   "outputs": [],
   "source": [
    "def get_initializer(hparams, scope):\n",
    "  initializer = tf.random_normal_initializer\n",
    "  if hparams.get(\"orth_init\"):\n",
    "    print(f\"orth init in scope {scope}\")\n",
    "    initializer = tf.compat.v1.orthogonal_initializer\n",
    "  return initializer\n",
    "\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02, hparams=None):\n",
    "    dtype = hparams.dtype if hparams else tf.float32\n",
    "\n",
    "    initializer = get_initializer(hparams, scope)\n",
    "    with tf.variable_scope(scope, dtype=dtype):\n",
    "        *start, nx = shape_list(x)\n",
    "        w = get_variable('w') or tf.get_variable('w', [1, nx, nf], initializer=initializer(w_init_stdev, dtype=dtype))\n",
    "        b = get_variable('b') or tf.get_variable('b', [nf], initializer=tf.constant_initializer(0, dtype=dtype))\n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c\n",
    "\n",
    "# this is a copy/paste -- we need to redefine \"attn\" so the \"conv1d\" defn'd above\n",
    "# is used\n",
    "\n",
    "def attn(x, scope, n_state, *, past, hparams):\n",
    "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
    "    assert n_state % hparams.n_head == 0\n",
    "    if past is not None:\n",
    "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "    def split_heads(x):\n",
    "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n",
    "\n",
    "    def merge_heads(x):\n",
    "        # Reverse of split_heads\n",
    "        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "    def mask_attn_weights(w):\n",
    "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w*b - tf.cast(65500 if w.dtype != tf.float32 else 1e10, w.dtype)*(1-b)\n",
    "        return w\n",
    "\n",
    "    def multihead_attn(q, k, v):\n",
    "        # q, k, v have shape [batch, heads, sequence, features]\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n",
    "\n",
    "        w = mask_attn_weights(w)\n",
    "        w = softmax(w)\n",
    "        w = dropout(w, hparams.attn_dropout)\n",
    "        a = tf.matmul(w, v)\n",
    "        return a\n",
    "\n",
    "    dtype = hparams.dtype if hparams else tf.float32\n",
    "    with tf.variable_scope(scope, dtype=dtype):\n",
    "        c = conv1d(x, 'c_attn', n_state*3, hparams=hparams)\n",
    "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
    "        present = tf.stack([k, v], axis=1)\n",
    "        if past is not None:\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            k = tf.concat([pk, k], axis=-2)\n",
    "            v = tf.concat([pv, v], axis=-2)\n",
    "        a = multihead_attn(q, k, v)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state, hparams=hparams)\n",
    "        a = dropout(a, hparams.res_dropout)\n",
    "        return a, present\n",
    "\n",
    "def attn_only_block(x, scope, *, past, hparams, do_input_norm=True):\n",
    "    dtype = hparams.dtype if hparams else tf.float32\n",
    "    do_resid = hparams.do_resid if hparams else True\n",
    "    print(f\"do_resid: {do_resid}\")\n",
    "    print(f\"do_input_norm: {do_input_norm}\")\n",
    "    with tf.variable_scope(scope, dtype=dtype):\n",
    "        nx = x.shape[-1].value\n",
    "\n",
    "        if do_input_norm:\n",
    "          x_attn_in = norm(x, 'ln_1', hparams=hparams)\n",
    "        else:\n",
    "          x_attn_in = x\n",
    "        a, present = attn(x_attn_in, 'attn', nx, past=past, hparams=hparams)\n",
    "        if do_resid:\n",
    "          x = x + a\n",
    "        else:\n",
    "          x = a\n",
    "\n",
    "        return x, present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jhVtpZeORzz"
   },
   "outputs": [],
   "source": [
    "def mlp_no_proj(x, scope, n_state, *, hparams, is_expansion=False):\n",
    "    dtype = hparams.dtype if hparams else tf.float32\n",
    "    with tf.variable_scope(scope, dtype=dtype):\n",
    "        nx = x.shape[-1].value\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state,\n",
    "                        w_init_stdev=0.02,\n",
    "                        hparams=hparams))\n",
    "        h = dropout(h, hparams.res_dropout)\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1PgebLgXeYO"
   },
   "outputs": [],
   "source": [
    "def selector(hparams, X, hparams_select, \n",
    "             layer_nums: list, \n",
    "             scope=\"model\", \n",
    "             reuse=tf.AUTO_REUSE,\n",
    "             norm_layers_after: bool=False,\n",
    "             use_mlp: bool=True,\n",
    "             resid_mlp: bool=True,\n",
    "             ):\n",
    "  results = {}\n",
    "\n",
    "  activations = model_activations(\n",
    "      hparams=hparams, hparams_select=hparams_select,\n",
    "       X=X, layer_nums=layer_nums,\n",
    "        norm_layers_after=norm_layers_after,\n",
    "        scope=scope, reuse=reuse,\n",
    "      )['activations']\n",
    "  \n",
    "  hs_select = []\n",
    "  for act_name, act in activations:\n",
    "    h_select, _ = attn_only_block(act, f'h_select_{act_name}', \n",
    "                                  hparams=hparams_select,\n",
    "                                  past=None,\n",
    "                                  do_input_norm=(not norm_layers_after))\n",
    "    h_select = norm(h_select, f'ln_2_select_{act_name}', hparams=hparams_select,)\n",
    "    hs_select.append(h_select)\n",
    "\n",
    "    h_select_in = tf.concat(hs_select, axis=-1)\n",
    "      \n",
    "    h_select_in_at_selection_ix = extract_selection_ix(X, h_select_in)['extracted']\n",
    "    \n",
    "  with tf.variable_scope(scope, reuse=reuse, dtype=hparams_select.dtype):\n",
    "    if use_mlp:\n",
    "      m = mlp_no_proj(h_select_in_at_selection_ix, \"select_mlp__\", len(layer_nums)*hparams.n_embd, hparams=hparams_select)\n",
    "      if resid_mlp:\n",
    "        h_select_in_at_selection_ix = m + h_select_in_at_selection_ix\n",
    "      else:\n",
    "        h_select_in_at_selection_ix = m\n",
    "      \n",
    "    \n",
    "    w_select = get_variable('w_select_')\n",
    "    if w_select is None:\n",
    "      initializer = get_initializer(hparams_select, scope)\n",
    "      w_select = tf.get_variable('w_select_', [len(layer_nums)*hparams.n_embd, 2],\n",
    "                                initializer=initializer(0.02, dtype=hparams.dtype))\n",
    "    \n",
    "    b_select = get_variable('b_select')\n",
    "    if b_select is None:\n",
    "      b_select = tf.get_variable('b_select', [2],\n",
    "                                initializer=tf.constant_initializer(0, dtype=hparams.dtype))\n",
    "      \n",
    "    select_logits = tf.matmul(h_select_in_at_selection_ix, w_select) + b_select\n",
    "\n",
    "  results['logits_select'] = select_logits\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define selector model hparams, add selector model to tf graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDiLrV9gJEL1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.training import HParams\n",
    "\n",
    "hparams_select_train = HParams(\n",
    "        n_vocab=hparams.n_vocab,\n",
    "        n_ctx=hparams.n_ctx,\n",
    "        n_embd=hparams.n_embd,\n",
    "        n_head=hparams.n_head,\n",
    "        n_layer=hparams.n_layer,\n",
    "        res_dropout=0.,\n",
    "        attn_dropout=0.,\n",
    "        dtype=tf.float32,\n",
    "        do_resid=False,\n",
    "        orth_init=True,\n",
    "    )\n",
    "\n",
    "hparams_select_eval = HParams(\n",
    "        n_vocab=hparams.n_vocab,\n",
    "        n_ctx=hparams.n_ctx,\n",
    "        n_embd=hparams.n_embd,\n",
    "        n_head=hparams.n_head,\n",
    "        n_layer=hparams.n_layer,\n",
    "        res_dropout=0,\n",
    "        attn_dropout=0,\n",
    "        dtype=tf.float32,\n",
    "        do_resid=False,\n",
    "        orth_init=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "colab_type": "code",
    "id": "mWpczpAiSy-C",
    "outputId": "48afdec7-fcaf-4f75-fd13-6295e7e53812"
   },
   "outputs": [],
   "source": [
    "layer_nums = [24-1, 36-1]\n",
    "norm_layers_after = False\n",
    "use_mlp = True\n",
    "resid_mlp = True\n",
    "\n",
    "with sess.as_default():\n",
    "  selection_step_train = selector(\n",
    "      hparams=hparams, hparams_select=hparams_select_train,\n",
    "       X=context_for_h, layer_nums=layer_nums,\n",
    "        norm_layers_after=norm_layers_after,\n",
    "        use_mlp=use_mlp, resid_mlp=resid_mlp\n",
    "      )\n",
    "  selection_step_eval = selector(\n",
    "    hparams=hparams, hparams_select=hparams_select_eval,\n",
    "      X=context_for_h, layer_nums=layer_nums,\n",
    "      norm_layers_after=norm_layers_after,\n",
    "      use_mlp=use_mlp, resid_mlp=resid_mlp\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load generator part of model into session from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "va2fSvBFwMdk",
    "outputId": "ee5caef6-7a45-4deb-a3cc-facb1fe5b7ec"
   },
   "outputs": [],
   "source": [
    "import tflex\n",
    "\n",
    "seed = None\n",
    "\n",
    "load_done = False\n",
    "\n",
    "while not load_done:\n",
    "  try:\n",
    "    tf.reset_default_graph()\n",
    "    sess = tflex.Session()\n",
    "\n",
    "    with sess.as_default():\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        context_for_h = tf.placeholder(tf.int32, [batch_size_for_h, None])\n",
    "        do_step_with_h = step_with_h(hparams, tokens=context_for_h, batch_size_=batch_size_for_h)\n",
    "\n",
    "        saver = tflex.Saver()\n",
    "        ckpt = tflex.latest_checkpoint(os.path.join('models', model_name))\n",
    "        print(f\"restoring checkpoint: {ckpt}\")\n",
    "        saver.restore(sess, ckpt)\n",
    "    load_done = True\n",
    "  except Exception as e:\n",
    "    print(f\"encountered {e}, retrying...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize weights of selector part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rRw8jlTiSzLx"
   },
   "outputs": [],
   "source": [
    "initialize_uninitialized(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more data munging -- this is unique to this modeling approach, not shared w/ the BERT one\n",
    "\n",
    "doing it at this point because we need to know how much data we have to set up AdamW correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATMVA7g7wUCj"
   },
   "outputs": [],
   "source": [
    "CONTINUATION_ONLY = True\n",
    "NO_TAGS = False\n",
    "\n",
    "UNAME_CHAR = \"友\"\n",
    "T_CHAR = \"职\"\n",
    "\n",
    "def strip_uname_tags(s, prompt, verbose=False):\n",
    "  post, optional_tchar, tagbody = s.partition(T_CHAR)\n",
    "  tags = [t for t in tagbody.split(\"#\") if len(t)>0]\n",
    "\n",
    "  stripped_tags = []\n",
    "  for t in tags:\n",
    "    uname_substr = UNAME_CHAR + t.rstrip(\" \")\n",
    "    if uname_substr not in prompt:\n",
    "        stripped_tags.append(\"#\" + t)\n",
    "    else:\n",
    "      if verbose:\n",
    "        print(f\"found {t} as {uname_substr}\")\n",
    "  \n",
    "  return post + optional_tchar + \"\".join(stripped_tags)\n",
    "\n",
    "train_data_for_selection = train_data_.copy()\n",
    "selector_input_continuation = train_data_for_selection.text.apply(lambda s: (s[:-2] if s.endswith(\"<|\") else s))\n",
    "\n",
    "if CONTINUATION_ONLY:\n",
    "  selector_input = selector_input_continuation.copy()\n",
    "else:\n",
    "  selector_input_prompt = train_data_for_selection.prompt.apply(lambda s: enc.decode(enc.encode(s)[-max_context_size:]))\n",
    "  selector_input = selector_input_prompt + selector_input_continuation\n",
    "\n",
    "if NO_TAGS:\n",
    "  selector_input = selector_input.apply(lambda s: s.partition(T_CHAR)[0].rstrip(\"\\n\\ufffa\\ufffb \") + T_CHAR)\n",
    "else:\n",
    "  selector_input.iloc[:] = [strip_uname_tags(s, prompt) for s, prompt in zip(selector_input, train_data_for_selection.prompt)]\n",
    "  selector_input = selector_input.apply(lambda s: s.rstrip(\"\\n\\ufffa\\ufffb \"))\n",
    "  \n",
    "selector_input = selector_input.apply(lambda s: enc.decode(enc.encode(s)[-(length-1):]))\n",
    "train_data_for_selection[\"selector_input\"] = selector_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "za0AH23EmtrT"
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3jdmMnY_F0Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data_for_selection, test_data_for_selection = train_test_split(train_data_for_selection, \n",
    "                                                                     test_size=TEST_SIZE, \n",
    "                                                                     stratify=train_data_for_selection.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43FAIu6X9puS"
   },
   "outputs": [],
   "source": [
    "train_data_for_selection[\"n_tokens\"] = train_data_for_selection[\"selector_input\"].apply(lambda s: len(enc.encode(s)))\n",
    "train_data_for_selection = train_data_for_selection.sort_values(by=\"n_tokens\")\n",
    "\n",
    "batches = [train_data_for_selection.iloc[row_ix:row_ix + batch_size_for_h, :]\n",
    "           for row_ix in range(0, len(train_data_for_selection), batch_size_for_h)]\n",
    "\n",
    "np.random.shuffle(batches)\n",
    "\n",
    "train_data_for_selection_final = pd.concat(batches, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t0jecu2DqetL"
   },
   "outputs": [],
   "source": [
    "def reshuffle_batches(train_data_for_selection):\n",
    "  batches = [train_data_for_selection.iloc[row_ix:row_ix + batch_size_for_h, :]\n",
    "           for row_ix in range(0, len(train_data_for_selection), batch_size_for_h)]\n",
    "\n",
    "  np.random.shuffle(batches)\n",
    "\n",
    "  return pd.concat(batches, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NDfkOcv_c2a"
   },
   "outputs": [],
   "source": [
    "test_data_for_selection[\"n_tokens\"] = test_data_for_selection[\"selector_input\"].apply(lambda s: len(enc.encode(s)))\n",
    "test_data_for_selection = test_data_for_selection.sort_values(by=\"n_tokens\")\n",
    "\n",
    "batches = [test_data_for_selection.iloc[row_ix:row_ix + batch_size_for_h, :]\n",
    "           for row_ix in range(0, len(test_data_for_selection), batch_size_for_h)]\n",
    "\n",
    "np.random.shuffle(batches)\n",
    "\n",
    "test_data_for_selection_final = pd.concat(batches, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EgzEb_NC0pg-",
    "outputId": "94ffc9d0-3eac-4410-af12-a4669c0b3039"
   },
   "outputs": [],
   "source": [
    "print(train_data_for_selection_final.shape)\n",
    "print(test_data_for_selection_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "E8nBI__FlFk1",
    "outputId": "00f0653c-bca2-4b1e-e2d2-1cb86205f4ee"
   },
   "outputs": [],
   "source": [
    "import tflex_sgdr\n",
    "\n",
    "n_batches_per_epoch = len(train_data_for_selection_final)//batch_size_for_h\n",
    "\n",
    "base_lr = 0.0001 * (1-0.25)/(1-0.175)\n",
    "min_lr = base_lr/20\n",
    "initial_period_steps=int(n_batches_per_epoch)\n",
    "\n",
    "try:\n",
    "  global_step = tf.get_variable('global_step_', shape=(), dtype=tf.int32, trainable=False, )\n",
    "except:\n",
    "  pass\n",
    "global_step.load(0, session=sess)\n",
    "\n",
    "lr = tflex_sgdr.sgdr_decay(base_lr, global_step, \n",
    "    initial_period_steps=initial_period_steps, \n",
    "    t_mul=1., m_mul=0.5\n",
    "    )\n",
    "lr = tf.maximum(lr, tf.constant(min_lr, dtype=lr.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNd3tdkiwYd3"
   },
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "  select_logits_train = selection_step_train['logits_select']\n",
    "  select_logits_eval = selection_step_eval['logits_select']\n",
    "  select_target = tf.placeholder(tf.int32, [batch_size_for_h], )\n",
    "\n",
    "  select_loss = tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(labels=select_target, logits=select_logits_train)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQBTzLq1waoD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.train import AdamOptimizer\n",
    "from tensorflow.contrib.opt import AdamWOptimizer\n",
    "\n",
    "weight_decay = 0.025\n",
    "opt = AdamWOptimizer(weight_decay=weight_decay*lr, learning_rate=lr)\n",
    "\n",
    "train_vars = [var for var in tf.trainable_variables() if \"select\" in var.name and \"ln_2_\" not in var.name]\n",
    "decay_vars = [var for var in train_vars if \"_scalars\" not in var.name and \"b_select\" not in var.name and \"ln_\" not in var.name]\n",
    "\n",
    "opt_gradients, opt_variables = zip(*opt.compute_gradients(select_loss, train_vars))\n",
    "opt_gradients, _ = tf.clip_by_global_norm(opt_gradients, 1.0)\n",
    "opt_apply = opt.apply_gradients(zip(opt_gradients, opt_variables), decay_var_list=decay_vars)\n",
    "\n",
    "initialize_uninitialized(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "h6SJ34swGFnA",
    "outputId": "4eecc2f5-b2d1-4687-f4c9-5e245083de4f"
   },
   "source": [
    "define model train and eval loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Khhx_XAUwyh4"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.special\n",
    "\n",
    "def train_selection(data, steps=None, start_ix=0, avg_loss_beta=0.98, show_mix=False, show_lr=False):\n",
    "  all_losses = []\n",
    "  running_loss = None\n",
    "  latest_mix = None\n",
    "\n",
    "  if show_mix:\n",
    "    with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE, dtype=hparams.dtype):\n",
    "      latest_mix_var = tf.unstack(tf.get_variable(\"w_select_scalars\"), axis=-1)\n",
    "      latest_mix_var_g = tf.get_variable(\"g_select_scalars\")\n",
    "\n",
    "  if steps is None:\n",
    "    steps = len(data)//batch_size_for_h\n",
    "\n",
    "  row_ix = start_ix*batch_size_for_h\n",
    "  for step_ix in range(start_ix, steps):\n",
    "    data_batch = data.iloc[row_ix:row_ix + batch_size_for_h, :]\n",
    "\n",
    "    batch_context = [enc.encode(text) + [SELECTION_TOK] for text in data_batch.selector_input.values]\n",
    "    max_tokens = max([len(toks) for toks in batch_context])\n",
    "    batch_context_ = [toks + [0 for _ in range(max_tokens-len(toks))] for toks in batch_context]\n",
    "    batch_context = batch_context_\n",
    "\n",
    "    batch_target = data_batch.target.values\n",
    "\n",
    "    batch_context_display = [(text[:20] + ('...' if len(text)>20 else ''), t) for text, t in zip(data_batch.selector_input.values,\n",
    "                                                                                              batch_target)]\n",
    "    print(f\"{step_ix}/{steps} | {sum(batch_target)}/{batch_size_for_h} pos | {batch_size_for_h} rows of {max_tokens} tokens\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    with sess.as_default():\n",
    "      try:\n",
    "        batch_loss, _ = sess.run([select_loss, opt_apply], feed_dict={context_for_h.name: batch_context, select_target.name: batch_target})\n",
    "      except tf.errors.InvalidArgumentError:\n",
    "        print(\"skipping\")\n",
    "        continue\n",
    "    t2 = time.time()\n",
    "    tdiff = t2 - t1\n",
    "\n",
    "    all_losses.append(batch_loss)\n",
    "\n",
    "    if running_loss is None:\n",
    "      if step_ix > 3:\n",
    "        running_loss = batch_loss\n",
    "        avg_display = f\"{running_loss:.4f}\"\n",
    "      else:\n",
    "        avg_display = \"None\"\n",
    "    else:\n",
    "      running_loss = (avg_loss_beta * running_loss) + ((1-avg_loss_beta) * batch_loss)\n",
    "      avg_display = f\"{running_loss:.4f}\"\n",
    "\n",
    "    print(f\"{step_ix}/{steps} | {tdiff:.2f}s  | loss={batch_loss:.4f}, avg={avg_display}\")\n",
    "    if show_mix and step_ix % 10 == 0:\n",
    "      latest_mixes_raw, latest_g = sess.run([latest_mix_var, latest_mix_var_g])\n",
    "      nmix=len(latest_mixes_raw)\n",
    "      nlayer = len(latest_mixes_raw[0])\n",
    "      \n",
    "      latest_g_formatted = [f\"{x: .4f}\" for x in latest_g]\n",
    "      latest_mixes_raw_formatted = [np.asarray([f\"{x: .4f}\" for x in mix]) for mix in latest_mixes_raw]\n",
    "\n",
    "      latest_mixes_normed = [scipy.special.softmax(mix, axis=0) for mix in latest_mixes_raw]\n",
    "      latest_mixes_normed_formatted = [np.asarray([f\"{x: .4f}\" for x in mix]) for mix in latest_mixes_normed]\n",
    "\n",
    "      for mix_ix in range(nmix):\n",
    "        print(f\"mix {mix_ix}:\")\n",
    "        print(f\"latest mix gain: {latest_g_formatted}\")\n",
    "        print(f'latest_mix_raw:\\n{latest_mixes_raw_formatted[mix_ix]}')\n",
    "        print(f'latest_mix_normed:\\n{latest_mixes_normed_formatted[mix_ix]}')\n",
    "\n",
    "    if show_lr:\n",
    "      print(f\"latest lr: {sess.run(lr):.4e}\")\n",
    "    print(\"\\n--------------\\n\")\n",
    "\n",
    "    row_ix += batch_size_for_h\n",
    "\n",
    "    current_step = global_step.eval(sess)\n",
    "    global_step.load(current_step+1, session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSwMB73Bw2CA"
   },
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "\n",
    "def predict_select(text_batch, threshold=0.5):\n",
    "  if len(text_batch) != batch_size_for_h:\n",
    "    raise ValueError(\"badlength\")\n",
    "  batch_context = []\n",
    "  for text in text_batch:\n",
    "    batch_context.append(enc.encode(text)[-(length-1):] + [SELECTION_TOK])\n",
    "  max_tokens = max([len(toks) for toks in batch_context])\n",
    "  batch_context_ = [toks + [0 for _ in range(max_tokens-len(toks))] for toks in batch_context]\n",
    "  batch_context = batch_context_\n",
    "\n",
    "  with sess.as_default():\n",
    "    logits = sess.run(select_logits_eval, feed_dict={context_for_h.name: batch_context})\n",
    "\n",
    "  probs = scipy.special.softmax(logits, axis=1)\n",
    "  results = {\"logits\": logits, \"probs\": probs, \"preds\": probs[:, 1]>threshold}\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFNo8btlgzKK"
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def eval_selection(data, steps=None, start_ix=0):\n",
    "  all_preds = []\n",
    "  all_probs = []\n",
    "  all_targets = []\n",
    "  all_row_ix = []\n",
    "\n",
    "  if steps is None:\n",
    "    steps = len(data)//batch_size_for_h\n",
    "\n",
    "  row_ix = start_ix*batch_size_for_h\n",
    "  for step_ix in range(start_ix, steps):\n",
    "    data_batch = data.iloc[row_ix:row_ix + batch_size_for_h, :]\n",
    "    \n",
    "    t1 = time.time()\n",
    "    try:\n",
    "      results_batch = predict_select(data_batch.selector_input.values)\n",
    "    except Exception as e:\n",
    "      print(f\"skipping batch ({e})\")\n",
    "      continue\n",
    "    t2 = time.time()\n",
    "    tdiff = t2 - t1\n",
    "\n",
    "    all_targets.extend(data_batch.target.values)\n",
    "    all_preds.extend(results_batch[\"preds\"])\n",
    "    all_probs.extend(results_batch[\"probs\"])\n",
    "    all_row_ix.extend(list(range(row_ix, row_ix + batch_size_for_h)))\n",
    "\n",
    "    accs = np.array(all_preds) == np.array(all_targets)\n",
    "    avg_acc = accs.mean()\n",
    "\n",
    "    tp = (np.array(all_targets)>0).sum()\n",
    "    pp = (np.array(all_preds)>0).sum()\n",
    "\n",
    "    assert len(all_targets) == len(all_preds)\n",
    "\n",
    "    print(f\"{step_ix}/{steps} | {tdiff:.2f}s | acc={avg_acc:.4f} | {tp}/{len(all_targets)} true pos | {pp}/{len(all_targets)} pred pos\")\n",
    "    print(\"\\n--------------\\n\")\n",
    "\n",
    "    row_ix += batch_size_for_h\n",
    "\n",
    "  all_probs = np.stack(all_probs)[:, 1]\n",
    "  return all_preds, all_probs, all_targets, all_row_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for several epochs\n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch_ix in range(n_epochs):\n",
    "  train_data_for_selection_final = reshuffle_batches(train_data_for_selection)\n",
    "  train_selection(train_data_for_selection_final, start_ix=0, show_mix=False, show_lr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "omNVk1f3W7yj",
    "outputId": "8e8c9af8-ae27-4c8b-bc36-6b28756afb5f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss, average_precision_score, accuracy_score\n",
    "\n",
    "all_preds, all_probs, all_targets, all_row_ix = eval_selection(test_data_for_selection_final, start_ix=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "vW-msGFeB2LH",
    "outputId": "af2bac38-328a-4c42-ebde-bb3ad8464f6c"
   },
   "outputs": [],
   "source": [
    "baserate_acc = max(test_data_for_selection_final.target.mean(), 1.-test_data_for_selection_final.target.mean())\n",
    "baserate_brier = brier_score_loss(test_data_for_selection_final.target, [test_data_for_selection_final.target.mean() for _ in range(len(test_data_for_selection_final.target))])\n",
    "baserate_AP = average_precision_score(test_data_for_selection_final.target, [test_data_for_selection_final.target.mean() for _ in range(len(test_data_for_selection_final.target))])\n",
    "\n",
    "print(f\"acc:   {accuracy_score(all_targets, all_preds):.3f} (vs {baserate_acc:.3f})\")\n",
    "print(f\"brier: {brier_score_loss(all_targets, all_probs):.3f} (vs {baserate_brier:.3f})\")\n",
    "print(f\"AP:    {average_precision_score(all_targets, all_probs):.3f} (vs {baserate_AP:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "otvj3o6CknPE",
    "outputId": "4a3582fa-fc78-4ef1-9888-cec65e36a948"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "ps, rs, ts = precision_recall_curve(all_targets, all_probs)\n",
    "\n",
    "plt.scatter(ps, rs, s=1, c=[0] + ts.tolist())\n",
    "plt.xlim([0, 1])\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "yjGSpaTn6TND",
    "outputId": "f8d5fa7a-0dcc-41ae-c86d-05d5b6af95ee"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "bins=np.linspace(0, 1, 9)\n",
    "\n",
    "ax[0].hist(all_probs, bins=bins, alpha=0.5, density=False, edgecolor='k')\n",
    "ax[0].set_xticks(bins)\n",
    "\n",
    "_, bins, _ = ax[1].hist(all_probs[pd.Series(all_targets) < 0.5], bins=bins, alpha=0.5, density=False, edgecolor='k')\n",
    "ax[1].hist(all_probs[pd.Series(all_targets) > 0.5], bins=bins, alpha=0.5, density=False, edgecolor='k');\n",
    "ax[1].set_xticks(bins);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "XAhpT4KemWLV",
    "outputId": "f51b0d12-cc71-4f5e-9936-b1a8b784021b"
   },
   "outputs": [],
   "source": [
    "calib_real = []\n",
    "calib_goal = []\n",
    "calib_width = []\n",
    "\n",
    "for e1, e2 in zip(bins[:-1], bins[1:]):\n",
    "  bin_probs = all_probs[(all_probs>=e1) & (all_probs < e2)]\n",
    "  bin_targs = pd.Series(all_targets)[(all_probs>=e1) & (all_probs < e2)].values\n",
    "\n",
    "  calib_real.append(bin_targs.mean())\n",
    "  calib_goal.append((e2+e1)/2)\n",
    "  calib_width.append(e2-e1)\n",
    "\n",
    "  print(f\"{e1:.0%} - {e2:.0%}:\\t{bin_targs.mean():.1%} (vs {(e2+e1)/2:.1%},  \\t{len(bin_targs)} examples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "nwspkEp5N2q4",
    "outputId": "c6bf7817-e291-466c-d692-17602f82f21f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.bar(calib_goal, calib_real, width=calib_width, edgecolor='k', alpha=0.5)\n",
    "plt.plot(calib_goal, calib_goal, marker='o', c='r', ls='--');\n",
    "plt.axis([0, 1, 0, 1]);\n",
    "plt.xticks(calib_goal)\n",
    "plt.yticks(calib_goal)\n",
    "plt.gca().set_aspect(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "9U-HhXJKpc_j",
    "outputId": "b0ca50aa-ab7c-4854-80ec-58d249599225"
   },
   "outputs": [],
   "source": [
    "print(f\"base rate: {pd.Series(all_targets).mean():.2%}\")\n",
    "\n",
    "pd.Series(all_probs).describe(percentiles=np.linspace(0, 1, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "DaGLBwsOXhPw",
    "outputId": "8d8ba51b-44c7-4e9a-b650-625b1a682795"
   },
   "outputs": [],
   "source": [
    "save_path = \"\" # wherever you want to put the .hdf5 checkpoint -- make sure directory exists already\n",
    "\n",
    "var_list = [var for var in tf.trainable_variables() if \"select\" in var.name]\n",
    "\n",
    "saver_tflex = tflex.Saver(\n",
    "            var_list=var_list,\n",
    "            max_to_keep=20,\n",
    "            keep_checkpoint_every_n_hours=20,\n",
    "            reshape=False)\n",
    "\n",
    "display(var_list)\n",
    "saver_tflex.save(sess, save_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_generator_to_select",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
