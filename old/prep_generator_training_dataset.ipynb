{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a scraped tumblr corpus you want to fine-tune GPT-2 on, this preps the data for training.\n",
    "\n",
    "To get a corpus, use a tumblr scraper such as https://github.com/bbolli/tumblr-utils\n",
    "\n",
    "After prepping the data, use `gpt-2/train.py` to finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lprint(s, prefix=\"\"):\n",
    "    print(f\"{prefix}{s}\", end=f\"\\n\\n{prefix}---------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTS_DIR = \"\"  # wherever the posts are, as .html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_CHAR = \"会\"\n",
    "A_CHAR = \"域\"\n",
    "T_CHAR = \"职\"\n",
    "\n",
    "UNAME_CHAR = \"友\"\n",
    "ORIG_POST_CHAR = \"翰\"\n",
    "\n",
    "EOT_FULL = \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reblogs_v5 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot of hardcoded magic numbers and stuff here -- i tuned a lot of stuff here \"to my taste\"\n",
    "# eg excluding reblog chains that have too much writing by others and not much by me\n",
    "\n",
    "def screen_for_inclusion(processed, post_metadata, \n",
    "                         skip_autoresponder_mention=False,\n",
    "                         is_v5_extension=False,\n",
    "                         has_v5_extension_permissiveness=False,\n",
    "                         suppress_logs=False):\n",
    "    body, _, tags = processed.partition(T_CHAR)\n",
    "    \n",
    "    ar_uname = UNAME_CHAR + map_uname(\"nostalgebraist-autoresponder\", uname_config=\"frank_v5_train\") + Q_CHAR\n",
    "    nost_uname = UNAME_CHAR + map_uname(\"nostalgebraist\", uname_config=\"frank_v5_train\") + Q_CHAR\n",
    "    nost_diagnostic = nost_uname[:-1]  # TODO: verify -- i had to fill this back in after thinking i'd removed it\n",
    "    \n",
    "    if skip_autoresponder_mention:\n",
    "        if \"Frank\" in processed:\n",
    "            f_ix=processed.index('Frank')\n",
    "            print(f\"rejecting for Frank mention: {processed[f_ix-10:f_ix+10]}\")\n",
    "            return False, 0, 0\n",
    "        if ar_uname in processed:\n",
    "            print(\"rejecting for AR post\")\n",
    "            return False, 0, 0\n",
    "    \n",
    "    if post_metadata[\"is_quotes\"] == True:\n",
    "        return False, 0, 0\n",
    "    \n",
    "    if ORIG_POST_CHAR in body:\n",
    "        other_body = \"\"\n",
    "        me_body = body.split(ORIG_POST_CHAR)[1]\n",
    "    elif A_CHAR in body:\n",
    "        me_body, other_body = \"\", \"\"\n",
    "        in_other = True\n",
    "        for ix, char in enumerate(body):\n",
    "            if char == Q_CHAR:\n",
    "                if body[ix-len(nost_diagnostic):ix+1] == nost_uname:\n",
    "                    in_other=False\n",
    "                else:\n",
    "                    in_other = True\n",
    "            if char == A_CHAR:\n",
    "                in_other = False\n",
    "            if in_other:\n",
    "                other_body += char\n",
    "            else:\n",
    "                me_body += char\n",
    "        #other_body, me_body = body.split(A_CHAR)[1]\n",
    "    else:\n",
    "        return False, 0, 0\n",
    "    \n",
    "    base_ratio_cutoff = 2\n",
    "    base_word_cutoff = 250\n",
    "    if is_v5_extension or has_v5_extension_permissiveness:\n",
    "        ratio_cutoff = 3\n",
    "        word_cutoff = 50\n",
    "    else:\n",
    "        ratio_cutoff = base_ratio_cutoff\n",
    "        word_cutoff = base_word_cutoff\n",
    "    \n",
    "    if len(me_body) < 10:\n",
    "        return False, len(me_body.split()), len(other_body.split())\n",
    "    if (len(other_body) / len(me_body)) > ratio_cutoff and len(me_body.split()) < word_cutoff:\n",
    "        if not suppress_logs:\n",
    "            print(f\"rejecting other_body {len(other_body.split())} words, me_body {len(me_body.split())} words\")\n",
    "        return False, len(me_body.split()), len(other_body.split())\n",
    "        \n",
    "    if is_v5_extension and screen_for_inclusion(\n",
    "        processed, post_metadata, skip_autoresponder_mention=True, is_v5_extension=False, suppress_logs=True\n",
    "    )[0]:\n",
    "        # print(f\"skipping other_body {len(other_body.split())} words, me_body {len(me_body.split())} words\")\n",
    "        return False, len(me_body.split()), len(other_body.split())\n",
    "    elif is_v5_extension:\n",
    "        print(f\"accepting other_body {len(other_body.split())} words, me_body {len(me_body.split())} words\")\n",
    "    \n",
    "    return True, len(me_body.split()), len(other_body.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def fix_p_in_h2_bug(raw_html):\n",
    "    return re.sub(r\"(<h2>.*)<p>(.*)</p>(.*</h2>)\", lambda m: \"\".join(m.groups()),  raw_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_all_posts(posts_dir=POSTS_DIR, \n",
    "                  existing_posts_dir=None, \n",
    "                  limit=None, \n",
    "                  is_v5_extension=False,\n",
    "                  has_v5_extension_permissiveness=False,\n",
    "                  is_reward=False,\n",
    "                  suppress_screener_logs=False,\n",
    "                  do_image_analysis=False,\n",
    "                  get_image_urls=False,\n",
    "                  use_cached_images=False,\n",
    "                  USE_A_CHAR_ALWAYS=True):\n",
    "    posts = []\n",
    "    post_fns = []\n",
    "    meta_counts = Counter()\n",
    "    all_meta_counts = Counter()\n",
    "    all_image_urls = set()\n",
    "    image_urls = set()\n",
    "    reply_urls_to_fns = defaultdict(set)\n",
    "            \n",
    "    # the next line refers to a (tiny) feature i haven't copied to the public github yet\n",
    "    # to include OCR'd images in training corpus using a cache to avoid repeat calls to rekognition\n",
    "    # so it's commented out\n",
    "    # \n",
    "    # user_defined_image_analysis = cached_image_analysis_fn if use_cached_images else IMAGE_ANALYSIS_FN\n",
    "    \n",
    "    user_defined_image_analysis = IMAGE_ANALYSIS_FN\n",
    "        \n",
    "    all_fns = os.listdir(posts_dir)\n",
    "    if existing_posts_dir is not None:\n",
    "        all_existing_fns = {fn for fn in os.listdir(existing_posts_dir) if fn.endswith(\".html\")}\n",
    "        all_fns = [fn for fn in all_fns if fn not in all_existing_fns]\n",
    "    \n",
    "    for ix, fn in enumerate(sorted(all_fns)):\n",
    "        if not fn.endswith(\".html\"):\n",
    "            continue\n",
    "    \n",
    "        with open(os.path.join(posts_dir, fn), \"r\") as f:\n",
    "            raw_html = f.read()\n",
    "            fixed_html = fix_p_in_h2_bug(raw_html)\n",
    "            soup = BeautifulSoup(fixed_html)\n",
    "\n",
    "        try:\n",
    "            # print(os.path.join(posts_dir, fn))\n",
    "            uname_config = \"frank_v5_operate\" if is_reward else \"frank_v5_train\"\n",
    "            processed, post_metadata = process_post(soup,\n",
    "                                                    uname_config=uname_config,\n",
    "                                                    do_image_analysis=do_image_analysis,\n",
    "                                                    get_image_urls=get_image_urls,\n",
    "                                                    user_defined_image_analysis=user_defined_image_analysis,\n",
    "                                                    debug=False)\n",
    "        except Exception as e:\n",
    "            print(f\"hit {e} on {fn}\")\n",
    "            continue\n",
    "            \n",
    "        for key in sorted(post_metadata.keys()):\n",
    "            if key not in {\"image_urls\", \"reply_post_url\"}:\n",
    "                all_meta_counts[key] += post_metadata[key]\n",
    "            if key == \"image_urls\":\n",
    "                all_image_urls.update(post_metadata[key])\n",
    "                \n",
    "        if USE_A_CHAR_ALWAYS:\n",
    "            processed = processed.replace(nost_uname, A_CHAR)\n",
    "           \n",
    "        passed_screen, words_me, words_other = screen_for_inclusion(\n",
    "            processed, post_metadata,\n",
    "            is_v5_extension=is_v5_extension,\n",
    "            has_v5_extension_permissiveness=has_v5_extension_permissiveness,\n",
    "            suppress_logs=suppress_screener_logs or is_reward)\n",
    "        \n",
    "        all_meta_counts[\"words_me\"] += words_me\n",
    "        all_meta_counts[\"words_other\"] += words_other\n",
    "        \n",
    "        if passed_screen or is_reward:\n",
    "            for key in sorted(post_metadata.keys()):\n",
    "                if key not in {\"image_urls\", \"reply_post_url\"}:\n",
    "                    meta_counts[key] += post_metadata[key]\n",
    "                if key == \"image_urls\":\n",
    "                    image_urls.update(post_metadata[key])\n",
    "                \n",
    "            meta_counts[\"words_me\"] += words_me\n",
    "            meta_counts[\"words_other\"] += words_other\n",
    "                \n",
    "            posts.append(processed)\n",
    "            post_fns.append(fn)\n",
    "            \n",
    "            if post_metadata[\"reply_post_url\"] is not None:\n",
    "                reply_urls_to_fns[post_metadata[\"reply_post_url\"]].add(fn)\n",
    "         \n",
    "        if ix % 500 == 0:\n",
    "            print(f\"{ix}/{len(all_fns)}\\n\")\n",
    "            for k in meta_counts.keys():\n",
    "                print(f\"incl_meta_counts[{k}]:\\t{meta_counts[k]}\\nall__meta_counts[{k}]:\\t{all_meta_counts[k]}\\n\")\n",
    "            if get_image_urls:\n",
    "                print(f\"n_images: {len(image_urls)}\")\n",
    "            print()\n",
    "            \n",
    "        if limit is not None:\n",
    "            if ix >= limit:\n",
    "                break\n",
    "     \n",
    "    if get_image_urls:\n",
    "        return posts, meta_counts, post_fns, image_urls, reply_urls_to_fns\n",
    "    else:\n",
    "        return posts, meta_counts, post_fns, reply_urls_to_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part 1: making a train corpus for the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posts, meta_counts, post_fns = get_all_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_me = meta_counts[\"words_me\"]\n",
    "total_other = meta_counts[\"words_other\"]\n",
    "\n",
    "print(f\"{total_me//1000}K words from me\")\n",
    "print(f\"{total_other//1000}K words from others\")\n",
    "print()\n",
    "print(f\"{total_me / (total_me + total_other):.1%} me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# review examples\n",
    "from textwrap import fill\n",
    "\n",
    "subset_review = [p for p in posts if not p.startswith(\"翰\")]\n",
    "\n",
    "for p in np.random.choice(subset_review, 10):\n",
    "    print(p)\n",
    "    print(\"\\n\\n\" + 20*\"~~~~~\" + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_string = \"\".join(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"\"  # fill in\n",
    "with open(TRAIN_DATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(posts_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "part 2: making a train corpus for the selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR_POSTS_DIR = \"\"  # wherever the scraped _bot_ posts are, as .html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_posts, meta_counts, post_fns, image_urls, reply_urls_to_fns = get_all_posts(\n",
    "    posts_dir=AR_POSTS_DIR, is_reward=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell assumes you have a \"reward\" file and are just adding to it\n",
    "# make it from scratch as an empty dict if you aren't\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"reward/reward.pkl.gz\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "ids_to_reward_data = data[\"ids_to_reward_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from reward_data import get_prompt_and_continuation_from_processed\n",
    "\n",
    "def post_from_id(id_: int, post_list: list):\n",
    "    ix = [i for i, _ in enumerate(post_ids) if _ == id_][0]\n",
    "    return post_list[ix]\n",
    "\n",
    "n_prompt_same = 0\n",
    "n_cont_same = 0\n",
    "\n",
    "new_ids_to_reward_data = {}\n",
    "\n",
    "for id_ in tqdm(set(ids_to_reward_data.keys()).intersection(post_ids)):\n",
    "    processed = post_from_id(id_, ar_posts)\n",
    "    prompt, continuation = get_prompt_and_continuation_from_processed(processed)\n",
    "    \n",
    "    new_row = {\"note_count\": ids_to_reward_data[id_][\"note_count\"]}\n",
    "    new_row[\"prompt\"] = prompt\n",
    "    new_row[\"continuation\"] = continuation\n",
    "    new_ids_to_reward_data[id_] = new_row\n",
    "    \n",
    "for id_ in tqdm(set(post_ids).difference(ids_to_reward_data.keys())):\n",
    "    processed = post_from_id(id_, ar_posts)\n",
    "    try:\n",
    "        prompt, continuation = get_prompt_and_continuation_from_processed(processed)\n",
    "    except Exception as e:\n",
    "        print(f\"skipping {id_}: {e}\")\n",
    "    \n",
    "    new_row = {\"note_count\": None}\n",
    "    new_row[\"prompt\"] = prompt\n",
    "    new_row[\"continuation\"] = continuation\n",
    "    new_ids_to_reward_data[id_] = new_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\"ids_to_reward_data\": new_ids_to_reward_data_bad_parses_removed, \"offset\": data[\"offset\"]}\n",
    "\n",
    "with open(\"reward/reward.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(new_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
