{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O7aEpnm-mjA_"
   },
   "source": [
    "Generator component for nostalgebraist-autoresponder.\n",
    "\n",
    "Needs a fine-tuned GPT-2 model appropriate for use with the rest of the nostalgebraist-autoresponder codebase, and a running instance of the bridge service to talk to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNtGV0SBo0MD"
   },
   "outputs": [],
   "source": [
    "# make sure you have a GPU with ~16GB memory, for 1558M\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CdrPoDRRoepx"
   },
   "outputs": [],
   "source": [
    "BRIDGE_SERVICE_URL = \"\"  # fill yours in -- make sure it's acccessible from wherever this is running\n",
    "generator_url = BRIDGE_SERVICE_URL + \"/pollgenerator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrlurDRCxKPk"
   },
   "outputs": [],
   "source": [
    "# cell for Colab-specific stuff, if you're on Colab\n",
    "\n",
    "# if you're using Google Colab, need to tell it not to use tf 2.x\n",
    "%tensorflow_version 1.x\n",
    "\n",
    "# if you're using Google Colab + Google Drive, mount and change dir\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd \"/content/drive/My Drive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5OnyDlmUvD9i"
   },
   "outputs": [],
   "source": [
    "# i assume you're now somewhere with a directory called \"gpt-2\" holding my gpt-2 fork\n",
    "# and your fine-tuned model\n",
    "%cd \"gpt-2\"\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "%pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_eTL65ryaM2"
   },
   "outputs": [],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import model, sample, encoder\n",
    "from load_dataset import load_dataset, Sampler\n",
    "\n",
    "model_name = \"\"  # fill in -- should be a directory under /models\n",
    "dataset = \"\"  # fill in -- should be a directory under /data\n",
    "\n",
    "EOT_WORKAROUND = True\n",
    "EOT_PREPEND = True\n",
    "\n",
    "eot_end_segment = \"<|endoftext|>\" if EOT_WORKAROUND else \"<|\"\n",
    "\n",
    "better_length = True\n",
    "\n",
    "# sets max context size, for long prompts we want to cut off to allow bot to write at least this many tokens\n",
    "required_continuation_room = 100 # 385 #500 \n",
    "\n",
    "batch_size = 4\n",
    "nsamples = batch_size\n",
    "\n",
    "seed = None\n",
    "\n",
    "if better_length:\n",
    "  length=825\n",
    "else:\n",
    "  length=625\n",
    "\n",
    "EXPERIMENTAL_TOP_P = False\n",
    "EXPERIMENTAL_MIDDLE_P_TWEAK = False\n",
    "\n",
    "temperature=0.95\n",
    "top_k=0\n",
    "top_p=0\n",
    "middle_p=0.85\n",
    "\n",
    "batch_size = 4\n",
    "nsamples = batch_size\n",
    "if better_length:\n",
    "    length=800\n",
    "else:\n",
    "    length=700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the selector is a model using activations from the generator?\n",
    "SELECT_VIA_GENERATOR = True\n",
    "\n",
    "# if so, it will run in this notebook -- need a checkpoint for it, and some hparams\n",
    "\n",
    "ckpt_select = \"\" # fill this in, should be a .hdf5 file you saved using `train_generator_to_select.ipynb`\n",
    "\n",
    "# fill these in -- should match hparams you used in `train_generator_to_select.ipynb`\n",
    "#\n",
    "# TODO: DRY (really these should go in a json file with the checkpoint or something)\n",
    "layer_nums = [24-1, 36-1]\n",
    "do_resid = False\n",
    "norm_layers_after = False\n",
    "use_mlp = True\n",
    "resid_mlp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvwsE8Say7zJ"
   },
   "outputs": [],
   "source": [
    "enc = encoder.get_encoder(model_name, eot_workaround=EOT_WORKAROUND)\n",
    "hparams = model.default_hparams()\n",
    "with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
    "    hparams.override_from_dict(json.load(f))\n",
    "\n",
    "hparams.set_hparam(\"attn_dropout\", 0)\n",
    "hparams.set_hparam(\"res_dropout\", 0)\n",
    "\n",
    "if dataset is not None:\n",
    "    chunks = load_dataset(enc, dataset, 50000)\n",
    "    data_sampler = Sampler(chunks)\n",
    "    start_token = None\n",
    "else:\n",
    "    context_tokens = None\n",
    "    start_token = enc.encoder['<|endoftext|>']\n",
    "if length is None:\n",
    "    length = hparams.n_ctx\n",
    "elif length > hparams.n_ctx:\n",
    "    raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5ycEnEjzNL4"
   },
   "outputs": [],
   "source": [
    "import tflex\n",
    "\n",
    "load_done = False\n",
    "\n",
    "while not load_done:\n",
    "  try:\n",
    "    tf.reset_default_graph()\n",
    "    if CPU:\n",
    "      sess = tf.Session()\n",
    "    else:\n",
    "      sess = tflex.Session()\n",
    "\n",
    "    with sess.as_default():\n",
    "        np.random.seed(seed)\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        if start_token is None:\n",
    "            context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "        else:\n",
    "            context = None\n",
    "\n",
    "        start_ix = 1 if start_token is not None else 0\n",
    "        output = sample.sample_sequence(stop_at_EOT=True, better_length=better_length,\n",
    "                                        eot_workaround=EOT_WORKAROUND,\n",
    "                                        enc=enc, \n",
    "            hparams=hparams, length=length,\n",
    "            start_token=start_token,\n",
    "            context=context,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k, top_p=top_p, \n",
    "            middle_p=middle_p\n",
    "        )[:, start_ix:]\n",
    "\n",
    "        saver = tflex.Saver()\n",
    "        ckpt = tflex.latest_checkpoint(os.path.join('models', model_name))\n",
    "\n",
    "        print(f\"restoring checkpoint: {ckpt}\")\n",
    "        saver.restore(sess, ckpt)\n",
    "    load_done = True\n",
    "  except Exception as e:\n",
    "    print(f\"encountered {e}, retrying...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIgrkPUboHgS"
   },
   "source": [
    "Business logic\n",
    "\n",
    "TODO: DRY (really this should be in a .py file somewhere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQOrZ7JXoRCh"
   },
   "source": [
    "TODO: clean up this stuff. Change to \"V5\" was long ago, blocks that check if we're in \"V5\" can be changed to assume it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6Ah-TVV8BIg"
   },
   "outputs": [],
   "source": [
    "# code related to generation, prompting, etc\n",
    "\n",
    "import re\n",
    "from textwrap import fill, wrap\n",
    "\n",
    "Q_CHAR = \"会\"\n",
    "A_CHAR = \"域\"\n",
    "T_CHAR = \"职\"\n",
    "ORIG_POST_CHAR = \"翰\"\n",
    "UNAME_CHAR = \"友\"\n",
    "\n",
    "Q_CHAR = \"会\"\n",
    "A_CHAR = \"域\"\n",
    "T_CHAR = \"职\"\n",
    "ORIG_POST_CHAR = \"翰\"\n",
    "UNAME_CHAR = \"友\"\n",
    "\n",
    "def get_prompted_continuation(prompt: str, \n",
    "                              continue_if_cut_off=False, \n",
    "                              max_continue_steps=12,\n",
    "                              verbose=False):\n",
    "    raw_text = prompt\n",
    "    raw_text = re.sub(r\"\\\\n\", \"\\n\", raw_text)\n",
    "    context_tokens = enc.encode(raw_text)\n",
    "\n",
    "    if better_length:\n",
    "      max_context_size = length - required_continuation_room\n",
    "    else:\n",
    "      max_context_size = hparams.n_ctx - length - 10\n",
    "    if len(context_tokens) > max_context_size:\n",
    "      orig_len = len(context_tokens)\n",
    "      context_tokens = context_tokens[-(max_context_size):]\n",
    "      print(f\"truncated {orig_len} to {len(context_tokens)}, max_context_size={max_context_size}\")\n",
    "    else:\n",
    "      print(f\"{len(context_tokens)} tokens can fit in max_context_size {max_context_size}\")\n",
    "\n",
    "    token_start_ix = len(context_tokens)\n",
    "\n",
    "    batch_context_tokens = [context_tokens for _ in range(batch_size)]\n",
    "    continuations = [[prompt] for _ in batch_context_tokens]\n",
    "    is_repeating = [False for _ in batch_context_tokens]\n",
    "    generated = 0\n",
    "    this_batch_continue_steps = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "      with sess.as_default():\n",
    "          out = sess.run(output, feed_dict={\n",
    "              context: batch_context_tokens\n",
    "          })[:, token_start_ix:]\n",
    "      for i in range(batch_size):\n",
    "          generated += 1\n",
    "          text = enc.decode(out[i])\n",
    "\n",
    "          if len(set(out[i])) >= 0.2*len(out[i]):\n",
    "            continuations[i].append(text)\n",
    "            is_repeating[i] = False\n",
    "          else:\n",
    "            continuations[i].append(\"\")\n",
    "            is_repeating[i] = True\n",
    "\n",
    "      if continue_if_cut_off:\n",
    "        next_prompts = [\"\".join(subtexts) for subtexts in continuations]\n",
    "        batch_context_tokens = [enc.encode(text)[-(max_context_size):] for text in next_prompts]\n",
    "        token_start_ix = len(batch_context_tokens[0])\n",
    "\n",
    "        next_prompts_contonly = [\"\".join(subtexts[1:]) for subtexts in continuations]\n",
    "        not_finished = [c for c, rep in zip(next_prompts_contonly, is_repeating)\n",
    "                          if (eot_end_segment not in c) and\n",
    "                          (not any([control_char in c for control_char in {Q_CHAR, A_CHAR, ORIG_POST_CHAR, UNAME_CHAR}])) and\n",
    "                          (len([char for char in c if char == T_CHAR]) < 2) and\n",
    "                          not rep\n",
    "                       ]\n",
    "        n_not_finished = len(not_finished)\n",
    "        more_needed =  n_not_finished > 0\n",
    "        more_permitted = this_batch_continue_steps < max_continue_steps\n",
    "\n",
    "        done = (not more_needed) or (not more_permitted)\n",
    "        if not done:\n",
    "          print(\"continuing within batch:\")\n",
    "          print(f\"\\t{n_not_finished}/{len(next_prompts)} unfinished\")\n",
    "          print(f\"\\t{this_batch_continue_steps}/{max_continue_steps} continue steps used\")\n",
    "\n",
    "          if verbose:\n",
    "            print(\"Using prompts:\")\n",
    "            for np in not_finished:\n",
    "              print(\"\\t\" + \"\\n\\t\".join(wrap(np, width=90)) + \"\\n\")\n",
    "\n",
    "          this_batch_continue_steps += 1\n",
    "      else:\n",
    "        done = True\n",
    "    \n",
    "    # cleanup\n",
    "    continuations_ = []\n",
    "    for subtexts in continuations:\n",
    "      text = \"\".join(subtexts[1:])  # don't return prompt as part of these\n",
    "      if not text.endswith(eot_end_segment) and eot_end_segment in text:\n",
    "        continuations_.append(text.split(eot_end_segment)[0] + eot_end_segment)\n",
    "      else:\n",
    "        continuations_.append(text)\n",
    "\n",
    "    return continuations_\n",
    "\n",
    "def get_prompted_continuation_with_retries_for_length(prompt: str, retry_if_under: int=60, best_of: int=3, prompt_from_dataset=False, verbose=False):\n",
    "    n_words = 0\n",
    "    n_retries = 0\n",
    "    best = {\"post\": \"\", \"tags\": []}\n",
    "\n",
    "    while (n_words < retry_if_under) and (n_retries < (best_of // batch_size)):\n",
    "        n_retries += 1\n",
    "        if prompt_from_dataset:\n",
    "                prompt = get_prompt_from_dataset(dataset)\n",
    "                if verbose:\n",
    "                    print(f\"Using prompt: {prompt}\")\n",
    "        if verbose:\n",
    "            print(f\"try #{n_retries} of {best_of // batch_size}...\")\n",
    "        continuations = get_prompted_continuation(prompt)\n",
    "        #continuation = continuations[0]\n",
    "        for continuation in continuations:\n",
    "          if prompt_from_dataset:\n",
    "              continuation = prompt + continuation\n",
    "              if continuation.startswith(\"endoftext|>\"):\n",
    "                  continuation = continuation[len(\"endoftext|>\"):]\n",
    "\n",
    "          parsed = parse_continuation(continuation, verbose=verbose)\n",
    "          n_words_latest = len(parsed['post'].split(\" \"))\n",
    "          if n_words_latest > n_words and continuation.endswith(eot_end_segment) and Q_CHAR not in continuation and A_CHAR not in continuation:\n",
    "              if verbose:\n",
    "                  print(f\"n_words {n_words_latest} beats previous high of {n_words}\")\n",
    "              n_words = n_words_latest\n",
    "              best = continuation\n",
    "\n",
    "    return [best]\n",
    "\n",
    "def get_prompted_continuation_with_length_proportional_sampling(prompt: str, avoid_if_under: int=20, best_of: int=3, verbose=False, prompt_from_dataset=False, return_all=False):\n",
    "    n_words_by_try = []\n",
    "    not_cut_off_by_try = []\n",
    "    no_control_chars = []\n",
    "\n",
    "    tries = []\n",
    "    best = {\"post\": \"\", \"tags\": []}\n",
    "\n",
    "    for n_retries in range(best_of // batch_size):\n",
    "        if prompt_from_dataset:\n",
    "                prompt = get_prompt_from_dataset(dataset)\n",
    "                if verbose:\n",
    "                    print(f\"Using prompt: {prompt}\")\n",
    "        if verbose:\n",
    "            print(f\"try #{n_retries} of {best_of // batch_size}...\")\n",
    "        continuations = get_prompted_continuation(prompt)\n",
    "        #continuation = continuations[0]\n",
    "        for continuation in continuations:\n",
    "          if prompt_from_dataset:\n",
    "              continuation = prompt + continuation\n",
    "              if continuation.startswith(\"endoftext|>\"):\n",
    "                  continuation = continuation[len(\"endoftext|>\"):]\n",
    "          parsed = parse_continuation(continuation, verbose=verbose)\n",
    "          n_words_latest = len(parsed['post'].split(\" \"))\n",
    "          if verbose:\n",
    "              print(f\"n_words {n_words_latest}\")\n",
    "          n_words_by_try.append(n_words_latest)\n",
    "          not_cut_off_by_try.append(continuation.endswith(eot_end_segment))\n",
    "          no_control_chars.append(Q_CHAR not in continuation and A_CHAR not in continuation)\n",
    "          tries.append(continuation)\n",
    "\n",
    "    keep = [(nw >= avoid_if_under) and (nco) and (ncc)\n",
    "              for nw, nco, ncc in zip(n_words_by_try, not_cut_off_by_try, no_control_chars)]\n",
    "    if any(keep):\n",
    "        n_words_by_try = [nw for nw, k in zip(n_words_by_try, keep) if k]\n",
    "        tries = [t for t, k in zip(tries, keep) if k]\n",
    "\n",
    "    probs = np.asarray(n_words_by_try) / sum(n_words_by_try)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"choosing between word counts {n_words_by_try}\\nprobs {probs}\")\n",
    "    choice_ix = np.random.choice(list(range(len(probs))), p=probs)\n",
    "\n",
    "    best = tries[choice_ix]\n",
    "    if verbose:\n",
    "        print(f\"chose #{choice_ix} ({n_words_by_try[choice_ix]} words, prob {probs[choice_ix]})\")\n",
    "\n",
    "    if return_all:\n",
    "        return tries\n",
    "    return [best]\n",
    "\n",
    "def parse_continuation(continuation: str, verbose=True):\n",
    "    if verbose:\n",
    "        print(f\"parsing the following raw output:\\n------------------\\n{fill(continuation)}\\n------------------\\n\")\n",
    "\n",
    "    # split out tags, if present\n",
    "    post, _ , tag_text = continuation.partition(T_CHAR)\n",
    "    tags = []\n",
    "    if len(tag_text) > 0:\n",
    "        tags = [s.rstrip(\" \") for s in tag_text.split(\"#\")]\n",
    "\n",
    "    post = post.lstrip(ORIG_POST_CHAR) # TODO: fix this in get_prompted_continuation_with_length_proportional_sampling\n",
    "    parsed = {\"post\": post, \"tags\": tags}\n",
    "    return parsed\n",
    "\n",
    "def get_prompt_from_dataset(dataset):\n",
    "    global data_sampler\n",
    "    if data_sampler is None:\n",
    "        print(\"getting data sampler...\")\n",
    "        chunks = load_dataset(enc, dataset, 50000)\n",
    "        data_sampler = Sampler(chunks)\n",
    "\n",
    "    segment = \"会\"\n",
    "    segments = []\n",
    "    #while segment[0]==\"会\": # V3\n",
    "    while segment[0] != \"翰\": # V4\n",
    "        while len(segments) == 0:\n",
    "            segments = enc.decode(data_sampler.sample(1024)).split(\"<|endoftext|>\")[1:]\n",
    "        segment = segments.pop()\n",
    "    if EOT_WORKAROUND:\n",
    "      if EOT_PREPEND:\n",
    "        segment = \"<|endoftext|>\" + segment\n",
    "        context_tokens = enc.encode(segment)[:5]\n",
    "      else:\n",
    "        context_tokens = enc.encode(segment)[:4]\n",
    "      print(f'using context_tokens {context_tokens} = {enc.decode(context_tokens)}')\n",
    "    else:\n",
    "      context_tokens = enc.encode(\"endoftext|>\" + segment)[:len(enc.encode(\"endoftext|>\"))+3]\n",
    "    prompt = enc.decode(context_tokens)\n",
    "    \n",
    "\n",
    "    return prompt\n",
    "\n",
    "def basic_n_continuations(prompt, N, \n",
    "                          avoid_if_under=20, \n",
    "                          avoid_if_cut_off=True, \n",
    "                          split_on_control_char=False,\n",
    "                          prompt_from_dataset=False,\n",
    "                          avoid_initial_blockquote=False,\n",
    "                          continue_if_cut_off=False,\n",
    "                          max_continue_steps=12,\n",
    "                          verbose=False):\n",
    "  if prompt_from_dataset:\n",
    "      prompt = get_prompt_from_dataset(dataset)\n",
    "\n",
    "  continuations = []\n",
    "  while len(continuations) < N:\n",
    "    print(f\"\\ncontinuing, have {len(continuations)} of {N}\\n\")      \n",
    "\n",
    "    this_batch_continuations = get_prompted_continuation(prompt, \n",
    "                                                         continue_if_cut_off=continue_if_cut_off,\n",
    "                                                         max_continue_steps=max_continue_steps,\n",
    "                                                         verbose=verbose,\n",
    "                                                         )\n",
    "\n",
    "    for c in this_batch_continuations:\n",
    "      if any([control_char in c for control_char in {Q_CHAR, A_CHAR, ORIG_POST_CHAR, UNAME_CHAR}]):\n",
    "        if split_on_control_char:\n",
    "          min_ix = min([i for i, char in enumerate(c) if char in {Q_CHAR, A_CHAR, ORIG_POST_CHAR, UNAME_CHAR}])\n",
    "          csub = c[:min_ix]\n",
    "          print(f\"splitting on control char:\")\n",
    "          print(f\"\\t{len(c)} chars, {len(c.split(' '))} words-->\\n\\t{len(csub)} chars, {len(csub.split(' '))} words\")\n",
    "          c = csub\n",
    "        else:\n",
    "          print(f\"rejecting because control char: \\n{fill(c)}\\n\")\n",
    "          continue\n",
    "\n",
    "      if len(c.split(\" \")) < avoid_if_under:\n",
    "        print(f\"rejecting because length under {avoid_if_under}: \\n{fill(c)}\\n\")\n",
    "      elif (not c.endswith(eot_end_segment)) and avoid_if_cut_off:\n",
    "        print(f\"rejecting because cut off: \\n{fill(c)}\\n\")\n",
    "      elif (c.startswith(\"<blockquote\")) and avoid_initial_blockquote:\n",
    "        print(f\"rejecting because initial blockquote: \\n{fill(c)}\\n\")\n",
    "      elif (len([char for char in c if char == T_CHAR]) >= 2):\n",
    "        print(f\"rejecting because multiple T_CHAR: \\n{fill(c)}\\n\")\n",
    "      else:\n",
    "        continuations.append(c)\n",
    "        \n",
    "  continuations_ = []\n",
    "  for continuation in continuations:\n",
    "    if prompt_from_dataset:\n",
    "      continuation = prompt + continuation\n",
    "      if EOT_PREPEND and continuation.startswith(\"<|endoftext|>\"):\n",
    "        continuation = continuation[len(\"<|endoftext|>\"):].lstrip(ORIG_POST_CHAR)\n",
    "    continuations_.append(continuation)\n",
    "  continuations = continuations_\n",
    "\n",
    "  return continuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the selector model uses generator activations (and thus runs in this notebook), we need to declare the code defining it.  \n",
    "\n",
    "TODO: DRY (this, too, *really* should be in a .py source file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static declarations for the selector model -- this is copy/paste ie repeating myself :(\n",
    "\n",
    "from textwrap import fill, wrap\n",
    "import scipy.special\n",
    "\n",
    "def show_note_probas(texts, probas, continuation_sentiments=None):\n",
    "    if continuation_sentiments is None:\n",
    "        for tpe, proba in zip(texts, probas):\n",
    "            print(f\"\\tpredicted prob: {proba:.1%}\\n\")\n",
    "            print(\"\\n~_~_~_~_~_\\n\")\n",
    "            print(\"\\n\".join(wrap(tpe, replace_whitespace=False)))\n",
    "            print(\"\\n~_~_~_~_~_\\n\")\n",
    "    else:\n",
    "        for tpe, proba, sent in zip(texts, probas, continuation_sentiments):\n",
    "            print(f\"\\tpredicted prob: {proba:.1%}, pos_sent {pos_sent(sent):.1%}\\n\")\n",
    "            print(\"\\n~_~_~_~_~_\\n\")\n",
    "            print(\"\\n\".join(wrap(tpe, replace_whitespace=False)))\n",
    "            print(\"\\n~_~_~_~_~_\\n\")\n",
    "\n",
    "if SELECT_VIA_GENERATOR:\n",
    "  from model import *\n",
    "\n",
    "  SELECTION_CHAR = \"<|endoftext|>\"\n",
    "  SELECTION_TOK = enc.encode(SELECTION_CHAR)[-1]\n",
    "\n",
    "  def extract_selection_ix(tokens, extract_from):\n",
    "    mask = tf.equal(tf.dtypes.cast(tokens, tf.int32), SELECTION_TOK)\n",
    "    extracted_ragged = tf.ragged.boolean_mask(extract_from, mask)\n",
    "    \n",
    "    row_lengths = extracted_ragged.row_lengths()\n",
    "    row_ixs = row_lengths-1\n",
    "    selection_ix = tf.stack([tf.range(0, batch_size, dtype=tf.int64), row_ixs], axis=1,)\n",
    "\n",
    "    extracted = tf.gather_nd(extracted_ragged.to_tensor(), selection_ix,  )\n",
    "\n",
    "    return {\"extracted\": extracted, \"selection_ix\": selection_ix}\n",
    "\n",
    "  def model_activations(hparams, X, hparams_select, \n",
    "                      layer_nums: list, \n",
    "                      norm_layers_after: bool=False,\n",
    "                      past=None, past_select=None,\n",
    "                      scope='model', reuse=tf.AUTO_REUSE):\n",
    "    activations = []\n",
    "    h_names = []\n",
    "    \n",
    "    dtype = hparams.dtype if hparams else tf.float32\n",
    "    with tf.variable_scope(scope, reuse=reuse, dtype=dtype):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wpe = get_variable('wpe') or tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.01, dtype=dtype))\n",
    "        wte = get_variable('wte') or tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.random_normal_initializer(stddev=0.02, dtype=dtype))\n",
    "        past_length = 0 if past is None else tf.shape(past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "            if layer in layer_nums:\n",
    "              h_name = f'h{layer}'\n",
    "              print(f'{h_name} found')\n",
    "              h_names.append(h_name)\n",
    "              activations.append(h)\n",
    "\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f', hparams=hparams)\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "\n",
    "        # activations\n",
    "        if norm_layers_after:\n",
    "          activations = [norm(act, f'ln_after_{act_name}', hparams=hparams_select)\n",
    "                         for act_name, act in zip(h_names, activations)]\n",
    "\n",
    "        results['activations'] = list(zip(h_names, activations))\n",
    "\n",
    "        return results\n",
    "\n",
    "  def get_initializer(hparams, scope):\n",
    "    initializer = tf.random_normal_initializer\n",
    "    if hparams.get(\"orth_init\"):\n",
    "      print(f\"orth init in scope {scope}\")\n",
    "      initializer = tf.compat.v1.orthogonal_initializer\n",
    "    return initializer\n",
    "\n",
    "  def conv1d(x, scope, nf, *, w_init_stdev=0.02, hparams=None):\n",
    "      dtype = hparams.dtype if hparams else tf.float32\n",
    "\n",
    "      initializer = get_initializer(hparams, scope)\n",
    "      with tf.variable_scope(scope, dtype=dtype):\n",
    "          *start, nx = shape_list(x)\n",
    "          w = get_variable('w') or tf.get_variable('w', [1, nx, nf], initializer=initializer(w_init_stdev, dtype=dtype))\n",
    "          b = get_variable('b') or tf.get_variable('b', [nf], initializer=tf.constant_initializer(0, dtype=dtype))\n",
    "          c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "          return c\n",
    "\n",
    "  # this is a copy/paste -- we need to redefine \"attn\" so the \"conv1d\" defn'd above\n",
    "  # is used\n",
    "\n",
    "  def attn(x, scope, n_state, *, past, hparams):\n",
    "      assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
    "      assert n_state % hparams.n_head == 0\n",
    "      if past is not None:\n",
    "          assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "      def split_heads(x):\n",
    "          # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "          return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n",
    "\n",
    "      def merge_heads(x):\n",
    "          # Reverse of split_heads\n",
    "          return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "      def mask_attn_weights(w):\n",
    "          # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "          _, _, nd, ns = shape_list(w)\n",
    "          b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "          b = tf.reshape(b, [1, 1, nd, ns])\n",
    "          w = w*b - tf.cast(65500 if w.dtype != tf.float32 else 1e10, w.dtype)*(1-b)\n",
    "          return w\n",
    "\n",
    "      def multihead_attn(q, k, v):\n",
    "          # q, k, v have shape [batch, heads, sequence, features]\n",
    "          w = tf.matmul(q, k, transpose_b=True)\n",
    "          w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n",
    "\n",
    "          w = mask_attn_weights(w)\n",
    "          w = softmax(w)\n",
    "          w = dropout(w, hparams.attn_dropout)\n",
    "          a = tf.matmul(w, v)\n",
    "          return a\n",
    "\n",
    "      dtype = hparams.dtype if hparams else tf.float32\n",
    "      with tf.variable_scope(scope, dtype=dtype):\n",
    "          c = conv1d(x, 'c_attn', n_state*3, hparams=hparams)\n",
    "          q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
    "          present = tf.stack([k, v], axis=1)\n",
    "          if past is not None:\n",
    "              pk, pv = tf.unstack(past, axis=1)\n",
    "              k = tf.concat([pk, k], axis=-2)\n",
    "              v = tf.concat([pv, v], axis=-2)\n",
    "          a = multihead_attn(q, k, v)\n",
    "          a = merge_heads(a)\n",
    "          a = conv1d(a, 'c_proj', n_state, hparams=hparams)\n",
    "          a = dropout(a, hparams.res_dropout)\n",
    "          return a, present\n",
    "\n",
    "  def attn_only_block(x, scope, *, past, hparams, do_input_norm=True):\n",
    "      dtype = hparams.dtype if hparams else tf.float32\n",
    "      do_resid = hparams.do_resid if hparams else True\n",
    "      print(f\"do_resid: {do_resid}\")\n",
    "      print(f\"do_input_norm: {do_input_norm}\")\n",
    "      with tf.variable_scope(scope, dtype=dtype):\n",
    "          nx = x.shape[-1].value\n",
    "\n",
    "          if do_input_norm:\n",
    "            x_attn_in = norm(x, 'ln_1', hparams=hparams)\n",
    "          else:\n",
    "            x_attn_in = x\n",
    "          a, present = attn(x_attn_in, 'attn', nx, past=past, hparams=hparams)\n",
    "          if do_resid:\n",
    "            x = x + a\n",
    "          else:\n",
    "            x = a\n",
    "\n",
    "          return x, present\n",
    "\n",
    "  def mlp_no_proj(x, scope, n_state, *, hparams, is_expansion=False):\n",
    "    dtype = hparams.dtype if hparams else tf.float32\n",
    "    with tf.variable_scope(scope, dtype=dtype):\n",
    "        nx = x.shape[-1].value\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state,\n",
    "                        w_init_stdev=0.02,\n",
    "                        hparams=hparams))\n",
    "        h = dropout(h, hparams.res_dropout)\n",
    "        return h\n",
    "\n",
    "  def selector(hparams, X, hparams_select, \n",
    "             layer_nums: list, \n",
    "             scope=\"model\", \n",
    "             reuse=tf.AUTO_REUSE,\n",
    "             norm_layers_after: bool=False,\n",
    "             use_mlp: bool=True,\n",
    "             resid_mlp: bool=True,\n",
    "             ):\n",
    "    results = {}\n",
    "\n",
    "    activations = model_activations(\n",
    "        hparams=hparams, hparams_select=hparams_select,\n",
    "        X=X, layer_nums=layer_nums,\n",
    "          norm_layers_after=norm_layers_after,\n",
    "          scope=scope, reuse=reuse,\n",
    "        )['activations']\n",
    "    \n",
    "    hs_select = []\n",
    "    for act_name, act in activations:\n",
    "      h_select, _ = attn_only_block(act, f'h_select_{act_name}', \n",
    "                                    hparams=hparams_select,\n",
    "                                    past=None,\n",
    "                                    do_input_norm=(not norm_layers_after))\n",
    "      h_select = norm(h_select, f'ln_2_select_{act_name}', hparams=hparams_select,)\n",
    "      hs_select.append(h_select)\n",
    "\n",
    "      h_select_in = tf.concat(hs_select, axis=-1)\n",
    "        \n",
    "      h_select_in_at_selection_ix = extract_selection_ix(X, h_select_in)['extracted']\n",
    "      \n",
    "    with tf.variable_scope(scope, reuse=reuse, dtype=hparams_select.dtype):\n",
    "      if use_mlp:\n",
    "        m = mlp_no_proj(h_select_in_at_selection_ix, \"select_mlp__\", len(layer_nums)*hparams.n_embd, hparams=hparams_select)\n",
    "        if resid_mlp:\n",
    "          h_select_in_at_selection_ix = m + h_select_in_at_selection_ix\n",
    "        else:\n",
    "          h_select_in_at_selection_ix = m\n",
    "        \n",
    "      \n",
    "      w_select = get_variable('w_select_')\n",
    "      if w_select is None:\n",
    "        initializer = get_initializer(hparams_select, scope)\n",
    "        w_select = tf.get_variable('w_select_', [len(layer_nums)*hparams.n_embd, 2],\n",
    "                                  initializer=initializer(0.02, dtype=hparams.dtype))\n",
    "      \n",
    "      b_select = get_variable('b_select')\n",
    "      if b_select is None:\n",
    "        b_select = tf.get_variable('b_select', [2],\n",
    "                                  initializer=tf.constant_initializer(0, dtype=hparams.dtype))\n",
    "        \n",
    "      select_logits = tf.matmul(h_select_in_at_selection_ix, w_select) + b_select\n",
    "\n",
    "    results['logits_select'] = select_logits\n",
    "\n",
    "    return results\n",
    "\n",
    "  def single_batch_predict_select(text_batch, threshold=0.5, debug=False):\n",
    "    if len(text_batch) != batch_size:\n",
    "      raise ValueError(\"badlength\")\n",
    "    batch_context = []\n",
    "    for text in text_batch:\n",
    "      for end_segment in {eot_end_segment, '<|'}:  # explicitly support old <| thing, for now\n",
    "          if text.endswith(end_segment):\n",
    "            text = text[:-len(end_segment)]\n",
    "      batch_context.append(enc.encode(text)[-(length-1):] + [SELECTION_TOK])\n",
    "      if debug:\n",
    "        print(f\"predicting on:\\n{enc.decode(batch_context[-1])}\\n\")\n",
    "    max_tokens = max([len(toks) for toks in batch_context])\n",
    "    batch_context_ = [toks + [0 for _ in range(max_tokens-len(toks))] for toks in batch_context]\n",
    "    batch_context = batch_context_\n",
    "\n",
    "    with sess.as_default():\n",
    "      logits = sess.run(select_logits, feed_dict={context_for_h.name: batch_context})\n",
    "\n",
    "    probs = scipy.special.softmax(logits, axis=1)[:, 1]\n",
    "    results = {\"logits\": logits, \"probs\": probs, \"preds\": probs>threshold}\n",
    "    return results\n",
    "\n",
    "  def predict_select(texts, threshold=0.5, debug=False):\n",
    "    batches = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "      batch = texts[i:i+batch_size]\n",
    "\n",
    "      while len(batch) != batch_size:\n",
    "        batch = batch + [batch[-1] for _ in range(batch_size - len(batch))]\n",
    "      batches.append(batch)\n",
    "\n",
    "    batch_results = [single_batch_predict_select(batch, threshold=threshold, debug=debug) for batch in batches]\n",
    "    \n",
    "    result_keys = batch_results[0].keys()\n",
    "    results = {k: np.concatenate([br[k] for br in batch_results])[:len(texts)]\n",
    "              for k in result_keys}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selector: add tf ops to current session, load checkpoint\n",
    "\n",
    "if SELECT_VIA_GENERATOR:\n",
    "  hparams_select = HParams(\n",
    "        n_vocab=hparams.n_vocab,\n",
    "        n_ctx=hparams.n_ctx,\n",
    "        n_embd=hparams.n_embd,\n",
    "        n_head=hparams.n_head,\n",
    "        n_layer=hparams.n_layer,\n",
    "        res_dropout=0,\n",
    "        attn_dropout=0,\n",
    "        dtype=tf.float32,\n",
    "        do_resid=do_resid,\n",
    "        orth_init=True,\n",
    "    )\n",
    "\n",
    "  with sess.as_default():\n",
    "    context_for_h = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    \n",
    "    selection_step = selector(\n",
    "    hparams=hparams, hparams_select=hparams_select,\n",
    "      X=context_for_h, layer_nums=layer_nums,\n",
    "      norm_layers_after=norm_layers_after,\n",
    "      use_mlp=use_mlp, resid_mlp=resid_mlp\n",
    "    )\n",
    "    \n",
    "    select_logits = selection_step['logits_select']\n",
    "\n",
    "  var_list = [var for var in tf.trainable_variables() if \"select\" in var.name]\n",
    "\n",
    "  done = False\n",
    "  while not done:\n",
    "    try:\n",
    "      tflex.load_variables(ckpt_select, session=sess, var_list=var_list)\n",
    "      done=True\n",
    "    except Exception as e:\n",
    "      print(f\"encountered {e}, retrying...\")\n",
    "\n",
    "  display(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvvpd8a88Qri"
   },
   "outputs": [],
   "source": [
    "# code setting up this notebook's interactions with the bridge service\n",
    "\n",
    "import requests, time\n",
    "RESULT_STACK = {}\n",
    "\n",
    "def serve_answer(data):\n",
    "  prompt = data[\"prompt\"]\n",
    "  kwargs = data[\"kwargs\"]\n",
    "  avoid_if_under = kwargs.get(\"avoid_if_under\", 20)\n",
    "  avoid_if_cut_off = kwargs.get(\"avoid_if_cut_off\", True)\n",
    "  split_on_control_char = kwargs.get(\"split_on_control_char\", False)\n",
    "  avoid_initial_blockquote = kwargs.get(\"avoid_initial_blockquote\", True)\n",
    "\n",
    "  continue_if_cut_off = kwargs.get(\"continue_if_cut_off\", True)\n",
    "  if continue_if_cut_off:\n",
    "    avoid_if_cut_off = False\n",
    "\n",
    "  if kwargs.get(\"V5\"):\n",
    "    try:\n",
    "      continuations = basic_n_continuations(prompt, N=kwargs['best_of'], \n",
    "                                            avoid_if_under=avoid_if_under, \n",
    "                                            avoid_if_cut_off=avoid_if_cut_off,\n",
    "                                            prompt_from_dataset=kwargs.get(\"prompt_from_dataset\"),\n",
    "                                            split_on_control_char=split_on_control_char,\n",
    "                                            avoid_initial_blockquote=avoid_initial_blockquote,\n",
    "                                            continue_if_cut_off=continue_if_cut_off)\n",
    "    except Exception as e:\n",
    "      print(f\"got {e}, trying without continue_if_cut_off\")\n",
    "      continuations = basic_n_continuations(prompt, N=kwargs['best_of'], \n",
    "                                            avoid_if_under=avoid_if_under, \n",
    "                                            avoid_if_cut_off=avoid_if_cut_off,\n",
    "                                            prompt_from_dataset=kwargs.get(\"prompt_from_dataset\"),\n",
    "                                            split_on_control_char=split_on_control_char,\n",
    "                                            avoid_initial_blockquote=avoid_initial_blockquote,\n",
    "                                            continue_if_cut_off=False)\n",
    "    parsed = data.copy()\n",
    "    parsed[\"continuations\"] = continuations\n",
    "  else:\n",
    "    kwargs = {k: v for k, v in kwargs.items() if k != \"V5\"}\n",
    "    continuations = get_prompted_continuation_with_length_proportional_sampling(prompt, **kwargs)\n",
    "    continuation = continuations[0]\n",
    "  \n",
    "    parsed = parse_continuation(continuation)\n",
    "    \n",
    "  return parsed\n",
    "\n",
    "def serve_textpost(data):\n",
    "  prompt = \"\"\n",
    "  kwargs = data[\"kwargs\"]\n",
    "  avoid_if_under = kwargs.get(\"avoid_if_under\", 30)\n",
    "  avoid_if_cut_off = kwargs.get(\"avoid_if_cut_off\", True)\n",
    "  split_on_control_char = kwargs.get(\"split_on_control_char\", True)\n",
    "  avoid_initial_blockquote = kwargs.get(\"avoid_initial_blockquote\", False)\n",
    "\n",
    "  continue_if_cut_off = kwargs.get(\"continue_if_cut_off\", True)\n",
    "  if continue_if_cut_off:\n",
    "    avoid_if_cut_off = False\n",
    "\n",
    "  if kwargs.get(\"V5\"):\n",
    "    try:\n",
    "      continuations = basic_n_continuations(prompt, \n",
    "                                            N=kwargs['best_of'], \n",
    "                                            avoid_if_under=avoid_if_under,\n",
    "                                            avoid_if_cut_off=avoid_if_cut_off,\n",
    "                                            split_on_control_char=split_on_control_char,\n",
    "                                            prompt_from_dataset=kwargs.get(\"prompt_from_dataset\"),\n",
    "                                            avoid_initial_blockquote=avoid_initial_blockquote,\n",
    "                                            continue_if_cut_off=continue_if_cut_off)\n",
    "    except Exception as e:\n",
    "      print(f\"got {e}, trying without continue_if_cut_off\")\n",
    "      continuations = basic_n_continuations(prompt, \n",
    "                                            N=kwargs['best_of'], \n",
    "                                            avoid_if_under=avoid_if_under,\n",
    "                                            avoid_if_cut_off=avoid_if_cut_off,\n",
    "                                            split_on_control_char=split_on_control_char,\n",
    "                                            prompt_from_dataset=kwargs.get(\"prompt_from_dataset\"),\n",
    "                                            avoid_initial_blockquote=avoid_initial_blockquote,\n",
    "                                            continue_if_cut_off=False)\n",
    "    parsed = data.copy()\n",
    "    parsed[\"continuations\"] = continuations\n",
    "  else:\n",
    "    kwargs = {k: v for k, v in kwargs.items() if k != \"V5\"}\n",
    "    continuations = get_prompted_continuation_with_retries_for_length(prompt, **kwargs)\n",
    "    continuation = continuations[0]\n",
    "  \n",
    "    parsed = parse_continuation(continuation)\n",
    "    \n",
    "  return parsed\n",
    "\n",
    "\n",
    "def poll():\n",
    "  global RESULT_STACK\n",
    "\n",
    "  r = requests.post(generator_url, json={\"results\": RESULT_STACK})\n",
    "\n",
    "  PROMPT_STACK = r.json()\n",
    "  \n",
    "  RESULT_STACK = {k: v for k, v in RESULT_STACK.items() if k in PROMPT_STACK}  # clean out already used results\n",
    "\n",
    "  for prompt_id, data in PROMPT_STACK.items():\n",
    "    print(\"generating...\")\n",
    "    if data[\"type\"] == \"answer\":\n",
    "      RESULT_STACK[prompt_id] = serve_answer(data)\n",
    "    elif data[\"type\"] == \"textpost\":\n",
    "      RESULT_STACK[prompt_id] = serve_textpost(data)\n",
    "\n",
    "  print(\"done generating for this poll\")\n",
    "\n",
    "  if len(PROMPT_STACK) > 0:\n",
    "    r = requests.post(generator_url, json={\"results\": RESULT_STACK})\n",
    "    time.sleep(1)\n",
    "\n",
    "import time\n",
    "\n",
    "def loop_poll(period=60):\n",
    "  global RESULT_STACK\n",
    "  while True:\n",
    "    try:\n",
    "      poll()\n",
    "    except Exception as e:\n",
    "      print(f\"{type(e)}: {e}\")\n",
    "      time.sleep(period*10)\n",
    "    if len(RESULT_STACK) == 0:\n",
    "      time.sleep(period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oa_meeUyogLq"
   },
   "source": [
    "Main loop of the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pd0PFHlwMBbl"
   },
   "outputs": [],
   "source": [
    "RESULT_STACK = {}\n",
    "\n",
    "loop_poll(period=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "generator",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
