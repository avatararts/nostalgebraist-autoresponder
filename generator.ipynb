{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generator",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7aEpnm-mjA_",
        "colab_type": "text"
      },
      "source": [
        "Generator component for nostalgebraist-autoresponder.\n",
        "\n",
        "Needs a fine-tuned GPT-2 model appropriate for use with the rest of the nostalgebraist-autoresponder codebase, and a running instance of the bridge service to talk to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNtGV0SBo0MD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure you have a GPU with ~16GB memory, for 1558M\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdrPoDRRoepx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BRIDGE_SERVICE_URL = \"\"  # fill yours in -- make sure it's acccessible from wherever this is running\n",
        "generator_url = BRIDGE_SERVICE_URL + \"/pollgenerator\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrlurDRCxKPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cell for Colab-specific stuff, if you're on Colab\n",
        "\n",
        "# if you're using Google Colab, need to tell it not to use tf 2.x\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "# if you're using Google Colab + Google Drive, mount and change dir\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/My Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OnyDlmUvD9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# i assume you're now somewhere with a directory called \"gpt-2\" holding my gpt-2 fork\n",
        "# and your fine-tuned model\n",
        "%cd \"gpt-2\"\n",
        "\n",
        "import os, sys\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "%pip install -r \"requirements.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_eTL65ryaM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fire\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import model, sample, encoder\n",
        "from load_dataset import load_dataset, Sampler\n",
        "\n",
        "model_name = \"\"  # fill in -- should be a directory under /models\n",
        "dataset = \"\"  # fill in -- should be a directory under /data\n",
        "\n",
        "better_length = True\n",
        "\n",
        "# sets max context size, for long prompts we want to cut off to allow bot to write at least this many tokens\n",
        "required_continuation_room = 100 # 385 #500 \n",
        "\n",
        "batch_size = 4\n",
        "nsamples = batch_size\n",
        "\n",
        "seed = None\n",
        "\n",
        "if better_length:\n",
        "  length=825\n",
        "else:\n",
        "  length=625\n",
        "\n",
        "EXPERIMENTAL_TOP_P = False\n",
        "EXPERIMENTAL_MIDDLE_P_TWEAK = False\n",
        "\n",
        "temperature=0.95\n",
        "top_k=0\n",
        "top_p=0\n",
        "middle_p=0.85\n",
        "\n",
        "batch_size = 4\n",
        "nsamples = batch_size\n",
        "if better_length:\n",
        "    length=800\n",
        "else:\n",
        "    length=700"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvwsE8Say7zJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc = encoder.get_encoder(model_name)\n",
        "hparams = model.default_hparams()\n",
        "with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
        "    hparams.override_from_dict(json.load(f))\n",
        "\n",
        "hparams.set_hparam(\"attn_dropout\", 0)\n",
        "hparams.set_hparam(\"res_dropout\", 0)\n",
        "\n",
        "if dataset is not None:\n",
        "    chunks = load_dataset(enc, dataset, 50000)\n",
        "    data_sampler = Sampler(chunks)\n",
        "    start_token = None\n",
        "else:\n",
        "    context_tokens = None\n",
        "    start_token = enc.encoder['<|endoftext|>']\n",
        "if length is None:\n",
        "    length = hparams.n_ctx\n",
        "elif length > hparams.n_ctx:\n",
        "    raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5ycEnEjzNL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tflex\n",
        "\n",
        "load_done = False\n",
        "\n",
        "while not load_done:\n",
        "  try:\n",
        "    tf.reset_default_graph()\n",
        "    if CPU:\n",
        "      sess = tf.Session()\n",
        "    else:\n",
        "      sess = tflex.Session()\n",
        "\n",
        "    with sess.as_default():\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "\n",
        "        if start_token is None:\n",
        "            context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        else:\n",
        "            context = None\n",
        "\n",
        "        start_ix = 1 if start_token is not None else 0\n",
        "        output = sample.sample_sequence(stop_at_EOT=True, better_length=better_length,\n",
        "                                        enc=enc, \n",
        "            hparams=hparams, length=length,\n",
        "            start_token=start_token,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k, top_p=top_p, \n",
        "            middle_p=middle_p\n",
        "        )[:, start_ix:]\n",
        "\n",
        "        saver = tflex.Saver()\n",
        "        ckpt = tflex.latest_checkpoint(os.path.join('models', model_name))\n",
        "        print(f\"restoring checkpoint: {ckpt}\")\n",
        "        saver.restore(sess, ckpt)\n",
        "    load_done = True\n",
        "  except Exception as e:\n",
        "    print(f\"encountered {e}, retrying...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIgrkPUboHgS",
        "colab_type": "text"
      },
      "source": [
        "Business logic -- really this should be in a .py file somewhere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQOrZ7JXoRCh",
        "colab_type": "text"
      },
      "source": [
        "TODO: clean up this stuff. Change to \"V5\" was long ago, blocks that check if we're in \"V5\" can be changed to assume it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6Ah-TVV8BIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from textwrap import fill, wrap\n",
        "\n",
        "Q_CHAR = \"会\"\n",
        "A_CHAR = \"域\"\n",
        "T_CHAR = \"职\"\n",
        "ORIG_POST_CHAR = \"翰\"\n",
        "UNAME_CHAR = \"友\"\n",
        "\n",
        "def get_prompted_continuation(prompt: str, \n",
        "                              continue_if_cut_off=False, \n",
        "                              max_continue_steps=12):\n",
        "    raw_text = prompt\n",
        "    raw_text = re.sub(r\"\\\\n\", \"\\n\", raw_text)\n",
        "    context_tokens = enc.encode(raw_text)\n",
        "\n",
        "    if better_length:\n",
        "      max_context_size = length - required_continuation_room\n",
        "    else:\n",
        "      max_context_size = hparams.n_ctx - length - 10\n",
        "    if len(context_tokens) > max_context_size:\n",
        "      orig_len = len(context_tokens)\n",
        "      context_tokens = context_tokens[-(max_context_size):]\n",
        "      print(f\"truncated {orig_len} to {len(context_tokens)}, max_context_size={max_context_size}\")\n",
        "    else:\n",
        "      print(f\"{len(context_tokens)} tokens can fit in max_context_size {max_context_size}\")\n",
        "\n",
        "    token_start_ix = len(context_tokens)\n",
        "\n",
        "    batch_context_tokens = [context_tokens for _ in range(batch_size)]\n",
        "    continuations = [[prompt] for _ in batch_context_tokens]\n",
        "    generated = 0\n",
        "    this_batch_continue_steps = 0\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "      with sess.as_default():\n",
        "          out = sess.run(output, feed_dict={\n",
        "              context: batch_context_tokens\n",
        "          })[:, token_start_ix:]\n",
        "      for i in range(batch_size):\n",
        "          generated += 1\n",
        "          text = enc.decode(out[i])\n",
        "\n",
        "          continuations[i].append(text)\n",
        "\n",
        "      if continue_if_cut_off:\n",
        "        next_prompts = [\"\".join(subtexts) for subtexts in continuations]\n",
        "        batch_context_tokens = [enc.encode(text)[-(max_context_size):] for text in next_prompts]\n",
        "        token_start_ix = len(batch_context_tokens[0])\n",
        "\n",
        "        next_prompts_contonly = [\"\".join(subtexts[1:]) for subtexts in continuations]\n",
        "        not_finished = [c for c in next_prompts_contonly \n",
        "                          if (\"<|\" not in c) and\n",
        "                          (not any([control_char in c for control_char in {Q_CHAR, A_CHAR, ORIG_POST_CHAR, UNAME_CHAR}]))\n",
        "                       ]\n",
        "        n_not_finished = len(not_finished)\n",
        "        more_needed =  n_not_finished > 0\n",
        "        more_permitted = this_batch_continue_steps < max_continue_steps\n",
        "\n",
        "        done = (not more_needed) or (not more_permitted)\n",
        "        if not done:\n",
        "          print(\"continuing within batch:\")\n",
        "          print(f\"\\t{n_not_finished}/{len(next_prompts)} unfinished\")\n",
        "          print(f\"\\t{this_batch_continue_steps}/{max_continue_steps} continue steps used\")\n",
        "\n",
        "          print(\"Using prompts:\")\n",
        "          for np in not_finished:\n",
        "            print(\"\\t\" + \"\\n\\t\".join(wrap(np, width=90)) + \"\\n\")\n",
        "\n",
        "          this_batch_continue_steps += 1\n",
        "      else:\n",
        "        done = True\n",
        "    \n",
        "    # cleanup\n",
        "    continuations_ = []\n",
        "    for subtexts in continuations:\n",
        "      text = \"\".join(subtexts[1:])  # don't return prompt as part of these\n",
        "      if not text.endswith(\"<|\") and \"<|endoftext|>\" in text:\n",
        "        continuations_.append(text.split(\"<|endoftext|>\")[0] + \"<|\")\n",
        "      else:\n",
        "        continuations_.append(text)\n",
        "\n",
        "    return continuations_\n",
        "\n",
        "def get_prompted_continuation_with_retries_for_length(prompt: str, retry_if_under: int=60, best_of: int=3, prompt_from_dataset=False, verbose=False):\n",
        "    n_words = 0\n",
        "    n_retries = 0\n",
        "    best = {\"post\": \"\", \"tags\": []}\n",
        "\n",
        "    while (n_words < retry_if_under) and (n_retries < (best_of // batch_size)):\n",
        "        n_retries += 1\n",
        "        if prompt_from_dataset:\n",
        "                prompt = get_prompt_from_dataset(dataset)\n",
        "                if verbose:\n",
        "                    print(f\"Using prompt: {prompt}\")\n",
        "        if verbose:\n",
        "            print(f\"try #{n_retries} of {best_of // batch_size}...\")\n",
        "        continuations = get_prompted_continuation(prompt)\n",
        "        for continuation in continuations:\n",
        "          if prompt_from_dataset:\n",
        "              continuation = prompt + continuation\n",
        "              if continuation.startswith(\"endoftext|>\"):\n",
        "                  continuation = continuation[len(\"endoftext|>\"):]\n",
        "\n",
        "          parsed = parse_continuation(continuation, verbose=verbose)\n",
        "          n_words_latest = len(parsed['post'].split(\" \"))\n",
        "          if n_words_latest > n_words and continuation.endswith(\"<|\") and Q_CHAR not in continuation and A_CHAR not in continuation:\n",
        "              if verbose:\n",
        "                  print(f\"n_words {n_words_latest} beats previous high of {n_words}\")\n",
        "              n_words = n_words_latest\n",
        "              best = continuation\n",
        "\n",
        "    return [best]\n",
        "\n",
        "def get_prompted_continuation_with_length_proportional_sampling(prompt: str, avoid_if_under: int=20, best_of: int=3, verbose=False, prompt_from_dataset=False, return_all=False):\n",
        "    n_words_by_try = []\n",
        "    not_cut_off_by_try = []\n",
        "    no_control_chars = []\n",
        "\n",
        "    tries = []\n",
        "    best = {\"post\": \"\", \"tags\": []}\n",
        "\n",
        "    for n_retries in range(best_of // batch_size):\n",
        "        if prompt_from_dataset:\n",
        "                prompt = get_prompt_from_dataset(dataset)\n",
        "                if verbose:\n",
        "                    print(f\"Using prompt: {prompt}\")\n",
        "        if verbose:\n",
        "            print(f\"try #{n_retries} of {best_of // batch_size}...\")\n",
        "        continuations = get_prompted_continuation(prompt)\n",
        "        #continuation = continuations[0]\n",
        "        for continuation in continuations:\n",
        "          if prompt_from_dataset:\n",
        "              continuation = prompt + continuation\n",
        "              if continuation.startswith(\"endoftext|>\"):\n",
        "                  continuation = continuation[len(\"endoftext|>\"):]\n",
        "          parsed = parse_continuation(continuation, verbose=verbose)\n",
        "          n_words_latest = len(parsed['post'].split(\" \"))\n",
        "          if verbose:\n",
        "              print(f\"n_words {n_words_latest}\")\n",
        "          n_words_by_try.append(n_words_latest)\n",
        "          not_cut_off_by_try.append(continuation.endswith(\"<|\"))\n",
        "          no_control_chars.append(Q_CHAR not in continuation and A_CHAR not in continuation)\n",
        "          tries.append(continuation)\n",
        "\n",
        "    keep = [(nw >= avoid_if_under) and (nco) and (ncc)\n",
        "              for nw, nco, ncc in zip(n_words_by_try, not_cut_off_by_try, no_control_chars)]\n",
        "    if any(keep):\n",
        "        n_words_by_try = [nw for nw, k in zip(n_words_by_try, keep) if k]\n",
        "        tries = [t for t, k in zip(tries, keep) if k]\n",
        "\n",
        "    probs = np.asarray(n_words_by_try) / sum(n_words_by_try)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"choosing between word counts {n_words_by_try}\\nprobs {probs}\")\n",
        "    choice_ix = np.random.choice(list(range(len(probs))), p=probs)\n",
        "\n",
        "    best = tries[choice_ix]\n",
        "    if verbose:\n",
        "        print(f\"chose #{choice_ix} ({n_words_by_try[choice_ix]} words, prob {probs[choice_ix]})\")\n",
        "\n",
        "    if return_all:\n",
        "        return tries\n",
        "    return [best]\n",
        "\n",
        "def parse_continuation(continuation: str, verbose=True):\n",
        "    if verbose:\n",
        "        print(f\"parsing the following raw output:\\n------------------\\n{fill(continuation)}\\n------------------\\n\")\n",
        "\n",
        "    # split out tags, if present\n",
        "    post, _ , tag_text = continuation.partition(T_CHAR)\n",
        "    tags = []\n",
        "    if len(tag_text) > 0:\n",
        "        tags = [s.rstrip(\" \") for s in tag_text.split(\"#\")]\n",
        "\n",
        "    post = post.lstrip(ORIG_POST_CHAR) # TODO: fix this in get_prompted_continuation_with_length_proportional_sampling\n",
        "    parsed = {\"post\": post, \"tags\": tags}\n",
        "    return parsed\n",
        "\n",
        "def get_prompt_from_dataset(dataset):\n",
        "    global data_sampler\n",
        "    if data_sampler is None:\n",
        "        print(\"getting data sampler...\")\n",
        "        chunks = load_dataset(enc, dataset, 50000)\n",
        "        data_sampler = Sampler(chunks)\n",
        "\n",
        "    segment = \"会\"\n",
        "    segments = []\n",
        "    #while segment[0]==\"会\": # V3\n",
        "    while segment[0] != \"翰\": # V4\n",
        "        while len(segments) == 0:\n",
        "            segments = enc.decode(data_sampler.sample(1024)).split(\"<|endoftext|>\")[1:]\n",
        "        segment = segments.pop()\n",
        "    context_tokens = enc.encode(\"endoftext|>\" + segment)[:len(enc.encode(\"endoftext|>\"))+3]\n",
        "    prompt = enc.decode(context_tokens)\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def basic_n_continuations(prompt, N, \n",
        "                          avoid_if_under=20, \n",
        "                          avoid_if_cut_off=True, \n",
        "                          split_on_control_char=False,\n",
        "                          prompt_from_dataset=False,\n",
        "                          avoid_initial_blockquote=False,\n",
        "                          continue_if_cut_off=False,\n",
        "                          max_continue_steps=12):\n",
        "  if prompt_from_dataset:\n",
        "      prompt = get_prompt_from_dataset(dataset)\n",
        "\n",
        "  continuations = []\n",
        "  while len(continuations) < N:\n",
        "    print(f\"\\ncontinuing, have {len(continuations)} of {N}\\n\")      \n",
        "\n",
        "    this_batch_continuations = get_prompted_continuation(prompt, \n",
        "                                                         continue_if_cut_off=continue_if_cut_off,\n",
        "                                                         max_continue_steps=max_continue_steps)\n",
        "\n",
        "    for c in this_batch_continuations:\n",
        "      if any([control_char in c for control_char in {Q_CHAR, A_CHAR, ORIG_POST_CHAR, UNAME_CHAR}]):\n",
        "        if split_on_control_char:\n",
        "          min_ix = min([i for i, char in enumerate(c) if char in {Q_CHAR, A_CHAR, ORIG_POST_CHAR, UNAME_CHAR}])\n",
        "          csub = c[:min_ix]\n",
        "          print(f\"splitting on control char:\")\n",
        "          print(f\"\\t{len(c)} chars, {len(c.split(' '))} words-->\\n\\t{len(csub)} chars, {len(csub.split(' '))} words\")\n",
        "          c = csub\n",
        "        else:\n",
        "          print(f\"rejecting because control char: \\n{fill(c)}\\n\")\n",
        "          continue\n",
        "\n",
        "      if len(c.split(\" \")) < avoid_if_under:\n",
        "        print(f\"rejecting because length under {avoid_if_under}: \\n{fill(c)}\\n\")\n",
        "      elif (not c.endswith(\"<|\")) and avoid_if_cut_off:\n",
        "        print(f\"rejecting because cut off: \\n{fill(c)}\\n\")\n",
        "      elif (c.startswith(\"<blockquote\")) and avoid_initial_blockquote:\n",
        "        print(f\"rejecting because initial blockquote: \\n{fill(c)}\\n\")\n",
        "      else:\n",
        "        continuations.append(c)\n",
        "        \n",
        "  for continuation in continuations:\n",
        "    if prompt_from_dataset:\n",
        "      continuation = prompt + continuation\n",
        "      if continuation.startswith(\"endoftext|>\"):\n",
        "        continuation = continuation[len(\"endoftext|>\"):].lstrip(ORIG_POST_CHAR)\n",
        "\n",
        "  return continuations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvvpd8a88Qri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests, time\n",
        "RESULT_STACK = {}\n",
        "\n",
        "def serve_answer(data):\n",
        "  prompt = data[\"prompt\"]\n",
        "  kwargs = data[\"kwargs\"]\n",
        "  avoid_if_under = kwargs.get(\"avoid_if_under\", 20)\n",
        "  avoid_if_cut_off = kwargs.get(\"avoid_if_cut_off\", True)\n",
        "  split_on_control_char = kwargs.get(\"split_on_control_char\", False)\n",
        "  avoid_initial_blockquote = kwargs.get(\"avoid_initial_blockquote\", True)\n",
        "\n",
        "  continue_if_cut_off = kwargs.get(\"continue_if_cut_off\", True)\n",
        "  if continue_if_cut_off:\n",
        "    avoid_if_cut_off = False\n",
        "\n",
        "  if kwargs.get(\"V5\"):\n",
        "    try:\n",
        "      continuations = basic_n_continuations(prompt, N=kwargs['best_of'], \n",
        "                                            avoid_if_under=avoid_if_under, \n",
        "                                            avoid_if_cut_off=avoid_if_cut_off,\n",
        "                                            prompt_from_dataset=kwargs.get(\"prompt_from_dataset\"),\n",
        "                                            split_on_control_char=split_on_control_char,\n",
        "                                            avoid_initial_blockquote=avoid_initial_blockquote,\n",
        "                                            continue_if_cut_off=continue_if_cut_off)\n",
        "    except Exception as e:\n",
        "      print(f\"got {e}, trying without continue_if_cut_off\")\n",
        "      continuations = basic_n_continuations(prompt, N=kwargs['best_of'], \n",
        "                                            avoid_if_under=avoid_if_under, \n",
        "                                            avoid_if_cut_off=avoid_if_cut_off,\n",
        "                                            prompt_from_dataset=kwargs.get(\"prompt_from_dataset\"),\n",
        "                                            split_on_control_char=split_on_control_char,\n",
        "                                            avoid_initial_blockquote=avoid_initial_blockquote,\n",
        "                                            continue_if_cut_off=False)\n",
        "    parsed = data.copy()\n",
        "    parsed[\"continuations\"] = continuations\n",
        "  else:\n",
        "    kwargs = {k: v for k, v in kwargs.items() if k != \"V5\"}\n",
        "    continuations = get_prompted_continuation_with_length_proportional_sampling(prompt, **kwargs)\n",
        "    continuation = continuations[0]\n",
        "  \n",
        "    parsed = parse_continuation(continuation)\n",
        "    \n",
        "  return parsed\n",
        "\n",
        "def serve_textpost(data):\n",
        "  prompt = \"\"\n",
        "  kwargs = data[\"kwargs\"]\n",
        "  avoid_if_under = kwargs.get(\"avoid_if_under\", 30)\n",
        "  avoid_if_cut_off = kwargs.get(\"avoid_if_cut_off\", True)\n",
        "  split_on_control_char = kwargs.get(\"split_on_control_char\", True)\n",
        "  avoid_initial_blockquote = kwargs.get(\"avoid_initial_blockquote\", False)\n",
        "\n",
        "  continue_if_cut_off = kwargs.get(\"continue_if_cut_off\", True)\n",
        "  if continue_if_cut_off:\n",
        "    avoid_if_cut_off = False\n",
        "\n",
        "  if kwargs.get(\"V5\"):\n",
        "    try:\n",
        "      continuations = basic_n_continuations(prompt, \n",
        "                                            N=kwargs['best_of'], \n",
        "                                            avoid_if_under=avoid_if_under,\n",
        "                                            avoid_if_cut_off=avoid_if_cut_off,\n",
        "                                            split_on_control_char=split_on_control_char,\n",
        "                                            prompt_from_dataset=kwargs.get(\"prompt_from_dataset\"),\n",
        "                                            avoid_initial_blockquote=avoid_initial_blockquote,\n",
        "                                            continue_if_cut_off=continue_if_cut_off)\n",
        "    except Exception as e:\n",
        "      print(f\"got {e}, trying without continue_if_cut_off\")\n",
        "      continuations = basic_n_continuations(prompt, \n",
        "                                            N=kwargs['best_of'], \n",
        "                                            avoid_if_under=avoid_if_under,\n",
        "                                            avoid_if_cut_off=avoid_if_cut_off,\n",
        "                                            split_on_control_char=split_on_control_char,\n",
        "                                            prompt_from_dataset=kwargs.get(\"prompt_from_dataset\"),\n",
        "                                            avoid_initial_blockquote=avoid_initial_blockquote,\n",
        "                                            continue_if_cut_off=False)\n",
        "    parsed = data.copy()\n",
        "    parsed[\"continuations\"] = continuations\n",
        "  else:\n",
        "    kwargs = {k: v for k, v in kwargs.items() if k != \"V5\"}\n",
        "    continuations = get_prompted_continuation_with_retries_for_length(prompt, **kwargs)\n",
        "    continuation = continuations[0]\n",
        "  \n",
        "    parsed = parse_continuation(continuation)\n",
        "    \n",
        "  return parsed\n",
        "\n",
        "\n",
        "def poll():\n",
        "  global RESULT_STACK\n",
        "\n",
        "  r = requests.post(generator_url, json={\"results\": RESULT_STACK})\n",
        "\n",
        "  PROMPT_STACK = r.json()\n",
        "  \n",
        "  RESULT_STACK = {k: v for k, v in RESULT_STACK.items() if k in PROMPT_STACK}  # clean out already used results\n",
        "\n",
        "  for prompt_id, data in PROMPT_STACK.items():\n",
        "    print(\"generating...\")\n",
        "    if data[\"type\"] == \"answer\":\n",
        "      RESULT_STACK[prompt_id] = serve_answer(data)\n",
        "    elif data[\"type\"] == \"textpost\":\n",
        "      RESULT_STACK[prompt_id] = serve_textpost(data)\n",
        "\n",
        "  print(\"done generating for this poll\")\n",
        "\n",
        "  if len(PROMPT_STACK) > 0:\n",
        "    r = requests.post(generator_url, json={\"results\": RESULT_STACK})\n",
        "    time.sleep(1)\n",
        "\n",
        "import time\n",
        "\n",
        "def loop_poll(period=60):\n",
        "  global RESULT_STACK\n",
        "  while True:\n",
        "    try:\n",
        "      poll()\n",
        "    except Exception as e:\n",
        "      print(f\"{type(e)}: {e}\")\n",
        "      time.sleep(period*10)\n",
        "    if len(RESULT_STACK) == 0:\n",
        "      time.sleep(period)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa_meeUyogLq",
        "colab_type": "text"
      },
      "source": [
        "Main loop of the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd0PFHlwMBbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RESULT_STACK = {}\n",
        "\n",
        "loop_poll(period=10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}