{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a scraped tumblr corpus you want to fine-tune GPT-2 on, this preps the data for training.\n",
    "\n",
    "To get a corpus, use a tumblr scraper such as https://gist.github.com/doersino/7e3e5db591e42bf543e1\n",
    "\n",
    "After prepping the data, use `gpt-2/train.py` to finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lprint(s, prefix=\"\"):\n",
    "    print(f\"{prefix}{s}\", end=f\"\\n\\n{prefix}---------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTS_DIR = \"\"  # wherever the posts are, as .html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_CHAR = \"会\"\n",
    "A_CHAR = \"域\"\n",
    "T_CHAR = \"职\"\n",
    "\n",
    "UNAME_CHAR = \"友\"\n",
    "ORIG_POST_CHAR = \"翰\"\n",
    "\n",
    "EOT_FULL = \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reblogs_v5 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot of hardcoded magic numbers and stuff here -- i tuned a lot of stuff here \"to my taste\"\n",
    "# eg excluding reblog chains that have too much writing by others and not much by me\n",
    "\n",
    "def screen_for_inclusion(processed, post_metadata):\n",
    "    body, _, tags = processed.partition(T_CHAR)\n",
    "    \n",
    "    if post_metadata[\"is_quotes\"] == True:\n",
    "        return False, 0, 0\n",
    "    \n",
    "    if ORIG_POST_CHAR in body:\n",
    "        other_body = \"\"\n",
    "        me_body = body.split(ORIG_POST_CHAR)[1]\n",
    "    elif A_CHAR in body:\n",
    "        me_body, other_body = \"\", \"\"\n",
    "        in_other = True\n",
    "        for char in body:\n",
    "            if char == Q_CHAR:\n",
    "                in_other = True\n",
    "            if char == A_CHAR:\n",
    "                in_other = False\n",
    "            if in_other:\n",
    "                other_body += char\n",
    "            else:\n",
    "                me_body += char\n",
    "    else:\n",
    "        return False, 0, 0\n",
    "    \n",
    "    if len(me_body) < 10:\n",
    "        return False, len(me_body.split()), len(other_body.split())\n",
    "    if (len(other_body) / len(me_body)) > 2 and len(me_body.split()) < 250:\n",
    "        print(f\"rejecting other_body {len(other_body.split())} words, me_body {len(me_body.split())} words\")\n",
    "        return False, len(me_body.split()), len(other_body.split())\n",
    "    \n",
    "    return True, len(me_body.split()), len(other_body.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_all_posts(posts_dir=POSTS_DIR, limit=None):\n",
    "    posts = []\n",
    "    post_fns = []\n",
    "    meta_counts = Counter()\n",
    "    all_meta_counts = Counter()\n",
    "            \n",
    "    all_fns = os.listdir(posts_dir)\n",
    "    for ix, fn in enumerate(all_fns):\n",
    "        if not fn.endswith(\".html\"):\n",
    "            continue\n",
    "    \n",
    "        with open(os.path.join(posts_dir, fn), \"r\") as f:\n",
    "            soup = BeautifulSoup(f)\n",
    "\n",
    "        try:\n",
    "            processed, post_metadata = process_post(soup, uname_config=\"frank_v5_train\", debug=False)\n",
    "        except Exception as e:\n",
    "            print(f\"hit {e} on {fn}\")\n",
    "            continue\n",
    "            \n",
    "        for key in sorted(post_metadata.keys()):\n",
    "            all_meta_counts[key] += post_metadata[key]\n",
    "           \n",
    "        passed_screen, words_me, words_other = screen_for_inclusion(processed, post_metadata)\n",
    "        \n",
    "        all_meta_counts[\"words_me\"] += words_me\n",
    "        all_meta_counts[\"words_other\"] += words_other\n",
    "        \n",
    "        if passed_screen:\n",
    "            for key in sorted(post_metadata.keys()):\n",
    "                meta_counts[key] += post_metadata[key]\n",
    "                \n",
    "            meta_counts[\"words_me\"] += words_me\n",
    "            meta_counts[\"words_other\"] += words_other\n",
    "                \n",
    "            posts.append(processed)\n",
    "            post_fns.append(fn)\n",
    "         \n",
    "        if ix % 500 == 0:\n",
    "            print(f\"{ix}/{len(all_fns)}\\n\")\n",
    "            for k in meta_counts.keys():\n",
    "                print(f\"incl_meta_counts[{k}]:\\t{meta_counts[k]}\\nall__meta_counts[{k}]:\\t{all_meta_counts[k]}\\n\")\n",
    "            print()\n",
    "            \n",
    "        if limit is not None:\n",
    "            if ix >= limit:\n",
    "                break\n",
    "            \n",
    "    return posts, meta_counts, post_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posts, meta_counts, post_fns = get_all_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_me = meta_counts[\"words_me\"]\n",
    "total_other = meta_counts[\"words_other\"]\n",
    "\n",
    "print(f\"{total_me//1000}K words from me\")\n",
    "print(f\"{total_other//1000}K words from others\")\n",
    "print()\n",
    "print(f\"{total_me / (total_me + total_other):.1%} me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# review examples\n",
    "from textwrap import fill\n",
    "\n",
    "subset_review = [p for p in posts if not p.startswith(\"翰\")]\n",
    "\n",
    "for p in np.random.choice(subset_review, 10):\n",
    "    print(p)\n",
    "    print(\"\\n\\n\" + 20*\"~~~~~\" + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_string = \"\".join(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"\"  # fill in\n",
    "with open(TRAIN_DATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(posts_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
